{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["d:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\AI-Project\\src\n"]}],"source":["%cd d:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\AI-Project\\src"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2834,"status":"ok","timestamp":1719024919996,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"HaVZ9PiiyiKu","outputId":"0ed83bfa-fa48-4fbc-8497-087d8ab67962"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","# %cd /content/drive/MyDrive/Colab Notebooks"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1719024919997,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"h-tVjAsGyiKx"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"u_GGEhMZzI1R"},"source":["# Part 1. Install necessary independences"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39200,"status":"ok","timestamp":1719024959194,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"vny0X0nHzIa1","outputId":"50717dcf-4619-4dff-c631-1f9deab39899"},"outputs":[],"source":["# !pip install wrds\n","# !pip install swig\n","# !pip install PyPortfolioOpt\n","# !pip install pyfolio-reloaded\n","# !pip install -q condacolab\n","# !pip install stockstats\n","# !pip install gym\n","# !pip install gymnasium\n","# !pip install stable_baselines3\n","# import condacolab\n","# condacolab.install()\n","# !apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":12220,"status":"ok","timestamp":1719024971407,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"W8hwJAqmyiKy"},"outputs":[],"source":["import sys\n","import os\n","import itertools\n","import pandas as pd\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","matplotlib.use('Agg')\n","import datetime\n","from plot import backtest_plot, get_baseline, backtest_stats\n","%matplotlib inline\n","from preprocessor.preprocessors import FeatureEngineer, data_split\n","from env.env_stocktrading import StockTradingEnv\n","from models.models import DRLAgent,DRLEnsembleAgent\n","from stable_baselines3.common.logger import configure\n","from models.algorithms import A2C, PPO, DDPG\n","\n","from pprint import pprint\n","\n","import itertools"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1719024971410,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"Dr9S-hhhyiKy"},"outputs":[],"source":["from config.config import (\n","    DATA_SAVE_DIR,\n","    TRAINED_MODEL_DIR,\n","    TENSORBOARD_LOG_DIR,\n","    RESULTS_DIR,\n","    INDICATORS,\n","    TRAIN_START_DATE,\n","    TRAIN_END_DATE,\n","    TEST_START_DATE,\n","    TEST_END_DATE,\n","    TRADE_START_DATE,\n","    TRADE_END_DATE,\n","    PPO_PARAMS,\n","    A2C_PARAMS,\n","    DDPG_PARAMS,\n","    check_and_make_directories\n",")\n","from config.config_tickers import DOW_30_TICKER\n","\n","check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"]},{"cell_type":"markdown","metadata":{"id":"za0b4ondyiKz"},"source":["# Part 2. Read data\n","\n","We first read the .csv file of our processed data into dataframe."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":681,"status":"ok","timestamp":1719024972074,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"Qv0GyAU6yiK1"},"outputs":[],"source":["processed_data = pd.read_csv(DATA_SAVE_DIR + 'processed_full.csv')\n","processed_data = processed_data.set_index(processed_data.columns[0])\n","processed_data.index.names = ['']"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":76,"status":"ok","timestamp":1719024972075,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"pXf3v9bATIjf","outputId":"bfe894ce-ce2c-45a5-eb44-dbf896cd1472"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train date: 2015-01-01 - 2020-01-01\n","Trade date: 2022-01-01 - 2024-06-01\n"]}],"source":["print(f\"Train date: {TRAIN_START_DATE} - {TRAIN_END_DATE}\")\n","print(f\"Trade date: {TRADE_START_DATE} - {TRADE_END_DATE}\")"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":69,"status":"ok","timestamp":1719024972076,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"3fGX9a6ymvyu","outputId":"4d045072-e446-41ac-9040-1c996d8507a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["36482\n"]}],"source":["train = data_split(processed_data, TRAIN_START_DATE,TRAIN_END_DATE)\n","print(len(train))"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":66,"status":"ok","timestamp":1719024972076,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"cKx8FjfF57Fq","outputId":"fa6a9b05-ab5b-4a9e-a6e4-7f7167555526"},"outputs":[{"data":{"text/plain":["'2020-01-01'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["TRAIN_END_DATE"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":66,"status":"ok","timestamp":1719024972078,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"fWHTfl7LyiK2","outputId":"0091bbac-fbb0-4783-8813-b8f8057924a3"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>tic</th>\n","      <th>open</th>\n","      <th>high</th>\n","      <th>low</th>\n","      <th>close</th>\n","      <th>volume</th>\n","      <th>day</th>\n","      <th>macd</th>\n","      <th>rsi_30</th>\n","      <th>cci_30</th>\n","      <th>dx_30</th>\n","      <th>vix</th>\n","      <th>turbulence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2015-01-02</td>\n","      <td>AAPL</td>\n","      <td>27.847500</td>\n","      <td>27.860001</td>\n","      <td>26.837500</td>\n","      <td>24.402172</td>\n","      <td>212818400.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-66.666667</td>\n","      <td>100.0</td>\n","      <td>17.790001</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>2015-01-02</td>\n","      <td>AMGN</td>\n","      <td>160.160004</td>\n","      <td>162.589996</td>\n","      <td>158.600006</td>\n","      <td>122.898552</td>\n","      <td>2605400.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-66.666667</td>\n","      <td>100.0</td>\n","      <td>17.790001</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>2015-01-02</td>\n","      <td>AXP</td>\n","      <td>93.169998</td>\n","      <td>93.940002</td>\n","      <td>92.139999</td>\n","      <td>80.772018</td>\n","      <td>2437500.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-66.666667</td>\n","      <td>100.0</td>\n","      <td>17.790001</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>2015-01-02</td>\n","      <td>BA</td>\n","      <td>131.070007</td>\n","      <td>131.839996</td>\n","      <td>129.089996</td>\n","      <td>113.657211</td>\n","      <td>4294200.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-66.666667</td>\n","      <td>100.0</td>\n","      <td>17.790001</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>2015-01-02</td>\n","      <td>CAT</td>\n","      <td>91.769997</td>\n","      <td>92.370003</td>\n","      <td>90.660004</td>\n","      <td>70.907639</td>\n","      <td>3767900.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-66.666667</td>\n","      <td>100.0</td>\n","      <td>17.790001</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         date   tic        open        high         low       close  \\\n","0  2015-01-02  AAPL   27.847500   27.860001   26.837500   24.402172   \n","0  2015-01-02  AMGN  160.160004  162.589996  158.600006  122.898552   \n","0  2015-01-02   AXP   93.169998   93.940002   92.139999   80.772018   \n","0  2015-01-02    BA  131.070007  131.839996  129.089996  113.657211   \n","0  2015-01-02   CAT   91.769997   92.370003   90.660004   70.907639   \n","\n","        volume  day  macd  rsi_30     cci_30  dx_30        vix  turbulence  \n","0  212818400.0  4.0   0.0     0.0 -66.666667  100.0  17.790001         0.0  \n","0    2605400.0  4.0   0.0     0.0 -66.666667  100.0  17.790001         0.0  \n","0    2437500.0  4.0   0.0     0.0 -66.666667  100.0  17.790001         0.0  \n","0    4294200.0  4.0   0.0     0.0 -66.666667  100.0  17.790001         0.0  \n","0    3767900.0  4.0   0.0     0.0 -66.666667  100.0  17.790001         0.0  "]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["train.head()"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":64,"status":"ok","timestamp":1719024972078,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"vr6XbDl9yiK2","outputId":"fb90bb74-4a9c-46c2-b533-5990f9eb6190"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>tic</th>\n","      <th>open</th>\n","      <th>high</th>\n","      <th>low</th>\n","      <th>close</th>\n","      <th>volume</th>\n","      <th>day</th>\n","      <th>macd</th>\n","      <th>rsi_30</th>\n","      <th>cci_30</th>\n","      <th>dx_30</th>\n","      <th>vix</th>\n","      <th>turbulence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1257</th>\n","      <td>2019-12-31</td>\n","      <td>UNH</td>\n","      <td>293.660004</td>\n","      <td>294.089996</td>\n","      <td>291.149994</td>\n","      <td>275.170197</td>\n","      <td>2224200.0</td>\n","      <td>1.0</td>\n","      <td>6.764926</td>\n","      <td>68.470048</td>\n","      <td>81.136801</td>\n","      <td>36.764747</td>\n","      <td>13.78</td>\n","      <td>3.600534</td>\n","    </tr>\n","    <tr>\n","      <th>1257</th>\n","      <td>2019-12-31</td>\n","      <td>V</td>\n","      <td>187.059998</td>\n","      <td>188.000000</td>\n","      <td>186.539993</td>\n","      <td>182.425278</td>\n","      <td>5273000.0</td>\n","      <td>1.0</td>\n","      <td>2.014772</td>\n","      <td>58.615893</td>\n","      <td>96.599562</td>\n","      <td>8.914116</td>\n","      <td>13.78</td>\n","      <td>3.600534</td>\n","    </tr>\n","    <tr>\n","      <th>1257</th>\n","      <td>2019-12-31</td>\n","      <td>VZ</td>\n","      <td>61.209999</td>\n","      <td>61.400002</td>\n","      <td>60.939999</td>\n","      <td>48.101662</td>\n","      <td>8466700.0</td>\n","      <td>1.0</td>\n","      <td>0.256966</td>\n","      <td>56.217642</td>\n","      <td>58.585691</td>\n","      <td>8.088789</td>\n","      <td>13.78</td>\n","      <td>3.600534</td>\n","    </tr>\n","    <tr>\n","      <th>1257</th>\n","      <td>2019-12-31</td>\n","      <td>WBA</td>\n","      <td>58.820000</td>\n","      <td>59.470001</td>\n","      <td>58.810001</td>\n","      <td>47.291977</td>\n","      <td>3305100.0</td>\n","      <td>1.0</td>\n","      <td>0.000871</td>\n","      <td>54.048382</td>\n","      <td>-6.217027</td>\n","      <td>16.696635</td>\n","      <td>13.78</td>\n","      <td>3.600534</td>\n","    </tr>\n","    <tr>\n","      <th>1257</th>\n","      <td>2019-12-31</td>\n","      <td>WMT</td>\n","      <td>39.833332</td>\n","      <td>39.933334</td>\n","      <td>39.396667</td>\n","      <td>37.065933</td>\n","      <td>14736000.0</td>\n","      <td>1.0</td>\n","      <td>0.045401</td>\n","      <td>51.364065</td>\n","      <td>-65.553194</td>\n","      <td>6.867213</td>\n","      <td>13.78</td>\n","      <td>3.600534</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            date  tic        open        high         low       close  \\\n","1257  2019-12-31  UNH  293.660004  294.089996  291.149994  275.170197   \n","1257  2019-12-31    V  187.059998  188.000000  186.539993  182.425278   \n","1257  2019-12-31   VZ   61.209999   61.400002   60.939999   48.101662   \n","1257  2019-12-31  WBA   58.820000   59.470001   58.810001   47.291977   \n","1257  2019-12-31  WMT   39.833332   39.933334   39.396667   37.065933   \n","\n","          volume  day      macd     rsi_30     cci_30      dx_30    vix  \\\n","1257   2224200.0  1.0  6.764926  68.470048  81.136801  36.764747  13.78   \n","1257   5273000.0  1.0  2.014772  58.615893  96.599562   8.914116  13.78   \n","1257   8466700.0  1.0  0.256966  56.217642  58.585691   8.088789  13.78   \n","1257   3305100.0  1.0  0.000871  54.048382  -6.217027  16.696635  13.78   \n","1257  14736000.0  1.0  0.045401  51.364065 -65.553194   6.867213  13.78   \n","\n","      turbulence  \n","1257    3.600534  \n","1257    3.600534  \n","1257    3.600534  \n","1257    3.600534  \n","1257    3.600534  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["train.tail()"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":63,"status":"ok","timestamp":1719024972079,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"F_C1MrsZyiK4","outputId":"c374c0b0-3282-43fc-8262-cb6afb40c54a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train shape: (36482, 14)\n"]}],"source":["print(f\"Train shape: {train.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"MJ7vpD24yiK4"},"source":["# Part 3. Construct the environment"]},{"cell_type":"markdown","metadata":{"id":"LqF5RPs_yiK4"},"source":["Calculate and specify the parameters we need for constructing the environment."]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":59,"status":"ok","timestamp":1719024972079,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"KJlR4E3cyiK5"},"outputs":[],"source":["INDICATORS = ['macd',\n","            'rsi_30',\n","            'cci_30',\n","            'dx_30']"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59,"status":"ok","timestamp":1719024972080,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"ajZ_0LYEyiK5","outputId":"f0a709a0-d485-4a36-8bfc-4da0674c443f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Stock Dimension: 29, State Space: 175\n"]}],"source":["stock_dimension = len(train.tic.unique())\n","state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n","print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":799,"status":"ok","timestamp":1719024972826,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"03-nyXneyiK5","outputId":"106471e5-0038-4966-8bbb-95f189c99fe6"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>tic</th>\n","      <th>open</th>\n","      <th>high</th>\n","      <th>low</th>\n","      <th>close</th>\n","      <th>volume</th>\n","      <th>day</th>\n","      <th>macd</th>\n","      <th>rsi_30</th>\n","      <th>cci_30</th>\n","      <th>dx_30</th>\n","      <th>vix</th>\n","      <th>turbulence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2015-01-02</td>\n","      <td>AAPL</td>\n","      <td>27.847500</td>\n","      <td>27.860001</td>\n","      <td>26.837500</td>\n","      <td>24.402172</td>\n","      <td>212818400.0</td>\n","      <td>4.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>-66.666667</td>\n","      <td>100.000000</td>\n","      <td>17.790001</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>2015-01-02</td>\n","      <td>AMGN</td>\n","      <td>160.160004</td>\n","      <td>162.589996</td>\n","      <td>158.600006</td>\n","      <td>122.898552</td>\n","      <td>2605400.0</td>\n","      <td>4.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>-66.666667</td>\n","      <td>100.000000</td>\n","      <td>17.790001</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>2015-01-02</td>\n","      <td>AXP</td>\n","      <td>93.169998</td>\n","      <td>93.940002</td>\n","      <td>92.139999</td>\n","      <td>80.772018</td>\n","      <td>2437500.0</td>\n","      <td>4.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>-66.666667</td>\n","      <td>100.000000</td>\n","      <td>17.790001</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>2015-01-02</td>\n","      <td>BA</td>\n","      <td>131.070007</td>\n","      <td>131.839996</td>\n","      <td>129.089996</td>\n","      <td>113.657211</td>\n","      <td>4294200.0</td>\n","      <td>4.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>-66.666667</td>\n","      <td>100.000000</td>\n","      <td>17.790001</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>2015-01-02</td>\n","      <td>CAT</td>\n","      <td>91.769997</td>\n","      <td>92.370003</td>\n","      <td>90.660004</td>\n","      <td>70.907639</td>\n","      <td>3767900.0</td>\n","      <td>4.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>-66.666667</td>\n","      <td>100.000000</td>\n","      <td>17.790001</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1257</th>\n","      <td>2019-12-31</td>\n","      <td>UNH</td>\n","      <td>293.660004</td>\n","      <td>294.089996</td>\n","      <td>291.149994</td>\n","      <td>275.170197</td>\n","      <td>2224200.0</td>\n","      <td>1.0</td>\n","      <td>6.764926</td>\n","      <td>68.470048</td>\n","      <td>81.136801</td>\n","      <td>36.764747</td>\n","      <td>13.780000</td>\n","      <td>3.600534</td>\n","    </tr>\n","    <tr>\n","      <th>1257</th>\n","      <td>2019-12-31</td>\n","      <td>V</td>\n","      <td>187.059998</td>\n","      <td>188.000000</td>\n","      <td>186.539993</td>\n","      <td>182.425278</td>\n","      <td>5273000.0</td>\n","      <td>1.0</td>\n","      <td>2.014772</td>\n","      <td>58.615893</td>\n","      <td>96.599562</td>\n","      <td>8.914116</td>\n","      <td>13.780000</td>\n","      <td>3.600534</td>\n","    </tr>\n","    <tr>\n","      <th>1257</th>\n","      <td>2019-12-31</td>\n","      <td>VZ</td>\n","      <td>61.209999</td>\n","      <td>61.400002</td>\n","      <td>60.939999</td>\n","      <td>48.101662</td>\n","      <td>8466700.0</td>\n","      <td>1.0</td>\n","      <td>0.256966</td>\n","      <td>56.217642</td>\n","      <td>58.585691</td>\n","      <td>8.088789</td>\n","      <td>13.780000</td>\n","      <td>3.600534</td>\n","    </tr>\n","    <tr>\n","      <th>1257</th>\n","      <td>2019-12-31</td>\n","      <td>WBA</td>\n","      <td>58.820000</td>\n","      <td>59.470001</td>\n","      <td>58.810001</td>\n","      <td>47.291977</td>\n","      <td>3305100.0</td>\n","      <td>1.0</td>\n","      <td>0.000871</td>\n","      <td>54.048382</td>\n","      <td>-6.217027</td>\n","      <td>16.696635</td>\n","      <td>13.780000</td>\n","      <td>3.600534</td>\n","    </tr>\n","    <tr>\n","      <th>1257</th>\n","      <td>2019-12-31</td>\n","      <td>WMT</td>\n","      <td>39.833332</td>\n","      <td>39.933334</td>\n","      <td>39.396667</td>\n","      <td>37.065933</td>\n","      <td>14736000.0</td>\n","      <td>1.0</td>\n","      <td>0.045401</td>\n","      <td>51.364065</td>\n","      <td>-65.553194</td>\n","      <td>6.867213</td>\n","      <td>13.780000</td>\n","      <td>3.600534</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>36482 rows Ã— 14 columns</p>\n","</div>"],"text/plain":["            date   tic        open        high         low       close  \\\n","0     2015-01-02  AAPL   27.847500   27.860001   26.837500   24.402172   \n","0     2015-01-02  AMGN  160.160004  162.589996  158.600006  122.898552   \n","0     2015-01-02   AXP   93.169998   93.940002   92.139999   80.772018   \n","0     2015-01-02    BA  131.070007  131.839996  129.089996  113.657211   \n","0     2015-01-02   CAT   91.769997   92.370003   90.660004   70.907639   \n","...          ...   ...         ...         ...         ...         ...   \n","1257  2019-12-31   UNH  293.660004  294.089996  291.149994  275.170197   \n","1257  2019-12-31     V  187.059998  188.000000  186.539993  182.425278   \n","1257  2019-12-31    VZ   61.209999   61.400002   60.939999   48.101662   \n","1257  2019-12-31   WBA   58.820000   59.470001   58.810001   47.291977   \n","1257  2019-12-31   WMT   39.833332   39.933334   39.396667   37.065933   \n","\n","           volume  day      macd     rsi_30     cci_30       dx_30        vix  \\\n","0     212818400.0  4.0  0.000000   0.000000 -66.666667  100.000000  17.790001   \n","0       2605400.0  4.0  0.000000   0.000000 -66.666667  100.000000  17.790001   \n","0       2437500.0  4.0  0.000000   0.000000 -66.666667  100.000000  17.790001   \n","0       4294200.0  4.0  0.000000   0.000000 -66.666667  100.000000  17.790001   \n","0       3767900.0  4.0  0.000000   0.000000 -66.666667  100.000000  17.790001   \n","...           ...  ...       ...        ...        ...         ...        ...   \n","1257    2224200.0  1.0  6.764926  68.470048  81.136801   36.764747  13.780000   \n","1257    5273000.0  1.0  2.014772  58.615893  96.599562    8.914116  13.780000   \n","1257    8466700.0  1.0  0.256966  56.217642  58.585691    8.088789  13.780000   \n","1257    3305100.0  1.0  0.000871  54.048382  -6.217027   16.696635  13.780000   \n","1257   14736000.0  1.0  0.045401  51.364065 -65.553194    6.867213  13.780000   \n","\n","      turbulence  \n","0       0.000000  \n","0       0.000000  \n","0       0.000000  \n","0       0.000000  \n","0       0.000000  \n","...          ...  \n","1257    3.600534  \n","1257    3.600534  \n","1257    3.600534  \n","1257    3.600534  \n","1257    3.600534  \n","\n","[36482 rows x 14 columns]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["train"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":119,"status":"ok","timestamp":1719024972827,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"Ju3GS6hAyiK6"},"outputs":[],"source":["buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n","num_stock_shares = [0] * stock_dimension\n","\n","env_kwargs = {\n","    \"hmax\": 100,\n","    \"initial_amount\": 1000000,\n","    \"num_stock_shares\": num_stock_shares,\n","    \"buy_cost_pct\": buy_cost_list,\n","    \"sell_cost_pct\": sell_cost_list,\n","    \"state_space\": state_space,\n","    \"stock_dim\": stock_dimension,\n","    \"tech_indicator_list\": INDICATORS,\n","    \"action_space\": stock_dimension,\n","    \"reward_scaling\": 1e-4\n","}\n","\n","e_train_gym = StockTradingEnv(df = train, **env_kwargs)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":119,"status":"ok","timestamp":1719024972828,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"0r5XCWh_yiK6","outputId":"fb6e9ab5-36f0-4e05-d2d3-8a5edd895754"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"]}],"source":["env_train, _ = e_train_gym.get_sb_env()\n","print(type(env_train))"]},{"cell_type":"markdown","metadata":{"id":"3hpTNtMyyiK6"},"source":["# Part 4. Train DRL Agents"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":103,"status":"ok","timestamp":1719024972830,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"-4y8yyBTyiK7"},"outputs":[],"source":["agent = DRLAgent(env = env_train)\n","\n","# Set the corresponding values to 'True' for the algorithms that you want to use\n","if_using_a2c = True\n","if_using_ddpg = True\n","if_using_ppo = True\n","if_using_ensemble = False"]},{"cell_type":"markdown","metadata":{"id":"JSb1iz1ryiK7"},"source":["### Agent 1: A2C"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1758,"status":"ok","timestamp":1719024974486,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"t98AV-wYyiK7","outputId":"591bf146-700c-45bb-a612-88cbe8417827"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n","Using cpu device\n","Logging to results/a2c\n"]}],"source":["agent = DRLAgent(env = env_train)\n","model_a2c = agent.get_model(\"a2c\", model_kwargs= A2C_PARAMS)\n","\n","if if_using_a2c:\n","  # set up logger\n","  tmp_path = RESULTS_DIR + 'a2c'\n","  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","  # Set new logger\n","  model_a2c.set_logger(new_logger_a2c)"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":176132,"status":"ok","timestamp":1719025150606,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"wK-0kRwFyiK7","outputId":"c0719ecd-9f39-4e76-faff-53755dbbba64"},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------\n","| time/                 |            |\n","|    fps                | 88         |\n","|    iterations         | 100        |\n","|    time_elapsed       | 5          |\n","|    total_timesteps    | 500        |\n","| train/                |            |\n","|    entropy_loss       | -41.2      |\n","|    explained_variance | -3.73      |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 99         |\n","|    policy_loss        | -2.04      |\n","|    reward             | 0.06526817 |\n","|    std                | 1          |\n","|    value_loss         | 0.375      |\n","--------------------------------------\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trained_a2c \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_a2c\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma2c\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m if_using_a2c \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\AI-Project\\src\\models\\models.py:113\u001b[0m, in \u001b[0;36mDRLAgent.train_model\u001b[1;34m(model, tb_log_name, total_timesteps)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\n\u001b[0;32m    111\u001b[0m     model, tb_log_name, total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m\n\u001b[0;32m    112\u001b[0m ):  \u001b[38;5;66;03m# this function is static method, so it can be called without creating an instance of the class\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTensorboardCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\AI-Project\\src\\models\\algorithms.py:207\u001b[0m, in \u001b[0;36mA2C.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfA2C,\n\u001b[0;32m    200\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    206\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfA2C:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:195\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[0;32m    193\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[1;32m--> 195\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\AI-Project\\src\\env\\env_stocktrading.py:348\u001b[0m, in \u001b[0;36mStockTradingEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    343\u001b[0m end_total_asset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[0;32m    344\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m1\u001b[39m : (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstock_dim \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)])\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstock_dim \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) : (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstock_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)])\n\u001b[0;32m    346\u001b[0m )\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masset_memory\u001b[38;5;241m.\u001b[39mappend(end_total_asset)\n\u001b[1;32m--> 348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdate_memory\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward \u001b[38;5;241m=\u001b[39m end_total_asset \u001b[38;5;241m-\u001b[39m begin_total_asset\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards_memory\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward)\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\AI-Project\\src\\env\\env_stocktrading.py:485\u001b[0m, in \u001b[0;36mStockTradingEnv._get_date\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_date\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    486\u001b[0m         date \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdate\u001b[38;5;241m.\u001b[39munique()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\pandas\\core\\series.py:2407\u001b[0m, in \u001b[0;36mSeries.unique\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:  \u001b[38;5;66;03m# pylint: disable=useless-parent-delegation\u001b[39;00m\n\u001b[0;32m   2345\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2346\u001b[0m \u001b[38;5;124;03m    Return unique values of Series object.\u001b[39;00m\n\u001b[0;32m   2347\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2405\u001b[0m \u001b[38;5;124;03m    Categories (3, object): ['a' < 'b' < 'c']\u001b[39;00m\n\u001b[0;32m   2406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\pandas\\core\\base.py:1025\u001b[0m, in \u001b[0;36mIndexOpsMixin.unique\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     result \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1025\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\pandas\\core\\algorithms.py:401\u001b[0m, in \u001b[0;36munique\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(values):\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    Return unique values based on a hash table.\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03m    array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munique_with_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\pandas\\core\\algorithms.py:440\u001b[0m, in \u001b[0;36munique_with_mask\u001b[1;34m(values, mask)\u001b[0m\n\u001b[0;32m    438\u001b[0m table \u001b[38;5;241m=\u001b[39m hashtable(\u001b[38;5;28mlen\u001b[39m(values))\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 440\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    441\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m _reconstruct_data(uniques, original\u001b[38;5;241m.\u001b[39mdtype, original)\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uniques\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["trained_a2c = agent.train_model(model=model_a2c,\n","                             tb_log_name='a2c',\n","                             total_timesteps=20000) if if_using_a2c else None"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":42,"status":"ok","timestamp":1719025150606,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"nkfSleGAyiK8"},"outputs":[],"source":["trained_a2c.save(TRAINED_MODEL_DIR + \"agent_a2c\") if if_using_a2c else None"]},{"cell_type":"markdown","metadata":{"id":"hT19ibDgyiK8"},"source":["### Agent 2: DDPG"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1719025150607,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"TGz6xWx6yiK8","outputId":"035eda31-ac37-4566-e867-2ed642330c56"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n","Using cpu device\n","Logging to results/ddpg\n"]}],"source":["agent = DRLAgent(env = env_train)\n","model_ddpg = agent.get_model(\"ddpg\", model_kwargs= DDPG_PARAMS)\n","\n","if if_using_ddpg:\n","  # set up logger\n","  tmp_path = RESULTS_DIR + 'ddpg'\n","  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","  # Set new logger\n","  model_ddpg.set_logger(new_logger_ddpg)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":293725,"status":"ok","timestamp":1719025444298,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"k0ErtaspyiK9","outputId":"cfc42aa4-c3e9-4b3b-f615-4b99656c7b21"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trained_ddpg \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_ddpg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mddpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m if_using_ddpg \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\AI-Project\\src\\models\\models.py:113\u001b[0m, in \u001b[0;36mDRLAgent.train_model\u001b[1;34m(model, tb_log_name, total_timesteps)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\n\u001b[0;32m    111\u001b[0m     model, tb_log_name, total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m\n\u001b[0;32m    112\u001b[0m ):  \u001b[38;5;66;03m# this function is static method, so it can be called without creating an instance of the class\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTensorboardCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\AI-Project\\src\\models\\algorithms.py:554\u001b[0m, in \u001b[0;36mDDPG.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDDPG,\n\u001b[0;32m    547\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    552\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    553\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDDPG:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\AI-Project\\src\\models\\algorithms.py:422\u001b[0m, in \u001b[0;36mTD3.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfTD3,\n\u001b[0;32m    415\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    420\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    421\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfTD3:\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 347\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\AI-Project\\src\\models\\algorithms.py:369\u001b[0m, in \u001b[0;36mTD3.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    367\u001b[0m noise \u001b[38;5;241m=\u001b[39m replay_data\u001b[38;5;241m.\u001b[39mactions\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnormal_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_policy_noise)\n\u001b[0;32m    368\u001b[0m noise \u001b[38;5;241m=\u001b[39m noise\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_noise_clip, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_noise_clip)\n\u001b[1;32m--> 369\u001b[0m next_actions \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_observations\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m noise)\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# TÃ­nh giÃ¡ trá»‹ next_q_values = giÃ¡ trá»‹ min tá»« Ä‘áº§u ra cá»§a táº¥t cáº£ cÃ¡c máº¡ng critic má»¥c tiÃªu\u001b[39;00m\n\u001b[0;32m    372\u001b[0m next_q_values \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target(replay_data\u001b[38;5;241m.\u001b[39mnext_observations, next_actions), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\stable_baselines3\\td3\\policies.py:78\u001b[0m, in \u001b[0;36mActor.forward\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;66;03m# assert deterministic, 'The TD3 actor only outputs deterministic actions'\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_extractor)\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\torch\\nn\\modules\\activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\torch\\nn\\functional.py:1500\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1498\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["trained_ddpg = agent.train_model(model=model_ddpg,\n","                            tb_log_name='ddpg',\n","                            total_timesteps=20000) if if_using_ddpg else None"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":89,"status":"ok","timestamp":1719025444300,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"ddzjqxQryiK9"},"outputs":[],"source":["trained_ddpg.save(TRAINED_MODEL_DIR + \"agent_ddpg\") if if_using_ddpg else None"]},{"cell_type":"markdown","metadata":{"id":"CycTaXfYyiK9"},"source":["### Agent 3: PPO"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":84,"status":"ok","timestamp":1719025444301,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"xXm8aN3pyiK_","outputId":"b04fb022-ea68-4cb0-aab7-ef89d84b3ce3"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n","Using cpu device\n","Logging to results/ppo\n"]}],"source":["agent = DRLAgent(env = env_train)\n","\n","model_ppo = agent.get_model(\"ppo\", model_kwargs = PPO_PARAMS)\n","\n","if if_using_ppo:\n","  # set up logger\n","  tmp_path = RESULTS_DIR + 'ppo'\n","  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n","  # Set new logger\n","  model_ppo.set_logger(new_logger_ppo)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":155471,"status":"ok","timestamp":1719025599701,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"QivMPu9-yiK_","outputId":"db9e50e2-c018-458d-fc4c-486c88ea8858"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trained_ppo \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_ppo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mppo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m if_using_ppo \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\AI-Project\\src\\models\\models.py:113\u001b[0m, in \u001b[0;36mDRLAgent.train_model\u001b[1;34m(model, tb_log_name, total_timesteps)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\n\u001b[0;32m    111\u001b[0m     model, tb_log_name, total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m\n\u001b[0;32m    112\u001b[0m ):  \u001b[38;5;66;03m# this function is static method, so it can be called without creating an instance of the class\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTensorboardCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\AI-Project\\src\\models\\algorithms.py:859\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    851\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    852\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    857\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:195\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[0;32m    193\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[1;32m--> 195\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\AI-Project\\src\\env\\env_stocktrading.py:341\u001b[0m, in \u001b[0;36mStockTradingEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mtic\u001b[38;5;241m.\u001b[39munique()) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    340\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mturbulence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrisk_indicator_col]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m end_total_asset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[0;32m    344\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m1\u001b[39m : (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstock_dim \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)])\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstock_dim \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) : (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstock_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)])\n\u001b[0;32m    346\u001b[0m )\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masset_memory\u001b[38;5;241m.\u001b[39mappend(end_total_asset)\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\AI-Project\\src\\env\\env_stocktrading.py:458\u001b[0m, in \u001b[0;36mStockTradingEnv._update_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_state\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;66;03m# for multiple stock\u001b[39;00m\n\u001b[0;32m    460\u001b[0m         state \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    461\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m    462\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclose\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    470\u001b[0m             )\n\u001b[0;32m    471\u001b[0m         )\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    474\u001b[0m         \u001b[38;5;66;03m# for single stock\u001b[39;00m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\pandas\\core\\series.py:2407\u001b[0m, in \u001b[0;36mSeries.unique\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:  \u001b[38;5;66;03m# pylint: disable=useless-parent-delegation\u001b[39;00m\n\u001b[0;32m   2345\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2346\u001b[0m \u001b[38;5;124;03m    Return unique values of Series object.\u001b[39;00m\n\u001b[0;32m   2347\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2405\u001b[0m \u001b[38;5;124;03m    Categories (3, object): ['a' < 'b' < 'c']\u001b[39;00m\n\u001b[0;32m   2406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\pandas\\core\\base.py:1025\u001b[0m, in \u001b[0;36mIndexOpsMixin.unique\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     result \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1025\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\pandas\\core\\algorithms.py:401\u001b[0m, in \u001b[0;36munique\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(values):\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    Return unique values based on a hash table.\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03m    array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munique_with_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\Learning\\University\\UIT\\AI\\Projects\\AI4Finance\\.conda\\AI4Finance\\lib\\site-packages\\pandas\\core\\algorithms.py:440\u001b[0m, in \u001b[0;36munique_with_mask\u001b[1;34m(values, mask)\u001b[0m\n\u001b[0;32m    438\u001b[0m table \u001b[38;5;241m=\u001b[39m hashtable(\u001b[38;5;28mlen\u001b[39m(values))\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 440\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    441\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m _reconstruct_data(uniques, original\u001b[38;5;241m.\u001b[39mdtype, original)\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uniques\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["trained_ppo = agent.train_model(model=model_ppo,\n","                             tb_log_name='ppo',\n","                             total_timesteps=20000) if if_using_ppo else None"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":241,"status":"ok","timestamp":1719025599896,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"LWZtNGL9yiLA"},"outputs":[],"source":["trained_ppo.save(TRAINED_MODEL_DIR + \"agent_ppo\") if if_using_ppo else None"]},{"cell_type":"markdown","metadata":{"id":"G7pOR9jLiuwM"},"source":["## Ensemble Strategy"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":241,"status":"ok","timestamp":1719025599897,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"id6SKw6MjGrL"},"outputs":[],"source":["ensemble_data = processed_data"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":240,"status":"ok","timestamp":1719025599897,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"wxrKUA6nyiLA"},"outputs":[],"source":["rebalance_window = 63\n","validation_window = 63\n","\n","env_ensemble_kwargs = {\n","    \"hmax\": 100,\n","    \"initial_amount\": 1000000,\n","    \"buy_cost_pct\": 0.001,\n","    \"sell_cost_pct\": 0.001,\n","    \"state_space\": state_space,\n","    \"stock_dim\": stock_dimension,\n","    \"tech_indicator_list\": INDICATORS,\n","    \"action_space\": stock_dimension,\n","    \"reward_scaling\": 1e-4,\n","    \"print_verbosity\":5\n","}\n","\n","ensemble_agent = DRLEnsembleAgent(df=ensemble_data,\n","                 train_period=(TRAIN_START_DATE,TRAIN_END_DATE),\n","                 val_test_period=(TRADE_START_DATE,TRADE_END_DATE),\n","                 rebalance_window=rebalance_window,\n","                 validation_window=validation_window,\n","                 **env_ensemble_kwargs) if if_using_ensemble else None"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":239,"status":"ok","timestamp":1719025599897,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"XyEMMXnjyiLA"},"outputs":[],"source":["A2C_model_kwargs = {\n","                    'n_steps': 5,\n","                    'ent_coef': 0.005,\n","                    'learning_rate': 0.0007\n","                    }\n","\n","PPO_model_kwargs = {\n","                    \"ent_coef\":0.01,\n","                    \"n_steps\": 2048,\n","                    \"learning_rate\": 0.00025,\n","                    \"batch_size\": 128\n","                    }\n","\n","DDPG_model_kwargs = {\n","                      #\"action_noise\":\"ornstein_uhlenbeck\",\n","                      \"buffer_size\": 10_000,\n","                      \"learning_rate\": 0.0005,\n","                      \"batch_size\": 64\n","                    }\n","\n","timesteps_dict = {'a2c' : 20000,\n","                 'ppo' : 20000,\n","                 'ddpg' : 20000,\n","                 }"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1641235,"status":"ok","timestamp":1719031715603,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"WduHxQWjyiLA","outputId":"487db054-6c1e-4b43-9aaf-90f9212f6649"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mKáº¿t quáº£ truyá»n trá»±c tuyáº¿n bá»‹ cáº¯t bá»›t Ä‘áº¿n 5000 dÃ²ng cuá»‘i.\u001b[0m\n","======a2c Validation from:  2022-07-06 to  2022-10-04\n","Episode: 1\n","a2c Sharpe Ratio:  -0.26117398891072374\n","======ddpg Training========\n","{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n","Using cuda device\n","Logging to tensorboard_log//ddpg/ddpg_252_1\n","Episode: 12\n","Episode: 13\n","Episode: 14\n","Episode: 15\n","day: 1888, episode: 15\n","begin_total_asset: 1000000.00\n","end_total_asset: 2426844.46\n","total_reward: 1426844.46\n","total_cost: 1651.84\n","total_trades: 18948\n","Sharpe: 0.699\n","=================================\n","-----------------------------------\n","| time/              |            |\n","|    episodes        | 4          |\n","|    fps             | 59         |\n","|    time_elapsed    | 126        |\n","|    total_timesteps | 7556       |\n","| train/             |            |\n","|    actor_loss      | -0.668     |\n","|    critic_loss     | 49.3       |\n","|    learning_rate   | 0.0005     |\n","|    n_updates       | 7455       |\n","|    reward          | -3.2919238 |\n","-----------------------------------\n","Episode: 16\n","Episode: 17\n","Episode: 18\n","Episode: 19\n","-----------------------------------\n","| time/              |            |\n","|    episodes        | 8          |\n","|    fps             | 59         |\n","|    time_elapsed    | 253        |\n","|    total_timesteps | 15112      |\n","| train/             |            |\n","|    actor_loss      | -3.19      |\n","|    critic_loss     | 0.939      |\n","|    learning_rate   | 0.0005     |\n","|    n_updates       | 15011      |\n","|    reward          | -3.2919238 |\n","-----------------------------------\n","Episode: 20\n","day: 1888, episode: 20\n","begin_total_asset: 1000000.00\n","end_total_asset: 2426844.46\n","total_reward: 1426844.46\n","total_cost: 1651.84\n","total_trades: 18948\n","Sharpe: 0.699\n","=================================\n","Episode: 21\n","======ddpg Validation from:  2022-07-06 to  2022-10-04\n","Episode: 1\n","ddpg Sharpe Ratio:  -0.022517072491992442\n","======ppo Training========\n","{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n","Using cuda device\n","Logging to tensorboard_log//ppo/ppo_252_1\n","Episode: 23\n","-----------------------------------\n","| time/              |            |\n","|    fps             | 113        |\n","|    iterations      | 1          |\n","|    time_elapsed    | 18         |\n","|    total_timesteps | 2048       |\n","| train/             |            |\n","|    reward          | -2.1326063 |\n","-----------------------------------\n","Episode: 24\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 110         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 37          |\n","|    total_timesteps      | 4096        |\n","| train/                  |             |\n","|    approx_kl            | 0.016353013 |\n","|    clip_fraction        | 0.211       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.2       |\n","|    explained_variance   | -0.0222     |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 5.49        |\n","|    n_updates            | 10          |\n","|    policy_gradient_loss | -0.0352     |\n","|    reward               | 0.1269926   |\n","|    std                  | 1           |\n","|    value_loss           | 14.6        |\n","-----------------------------------------\n","Episode: 25\n","day: 1888, episode: 25\n","begin_total_asset: 1000000.00\n","end_total_asset: 1668583.22\n","total_reward: 668583.22\n","total_cost: 304117.86\n","total_trades: 50671\n","Sharpe: 0.497\n","=================================\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 107         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 57          |\n","|    total_timesteps      | 6144        |\n","| train/                  |             |\n","|    approx_kl            | 0.015330994 |\n","|    clip_fraction        | 0.151       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.2       |\n","|    explained_variance   | -0.0103     |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 9.37        |\n","|    n_updates            | 20          |\n","|    policy_gradient_loss | -0.0304     |\n","|    reward               | 0.5670317   |\n","|    std                  | 1           |\n","|    value_loss           | 15.9        |\n","-----------------------------------------\n","Episode: 26\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 104         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 78          |\n","|    total_timesteps      | 8192        |\n","| train/                  |             |\n","|    approx_kl            | 0.018243263 |\n","|    clip_fraction        | 0.24        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.3       |\n","|    explained_variance   | -0.0335     |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 3.59        |\n","|    n_updates            | 30          |\n","|    policy_gradient_loss | -0.0328     |\n","|    reward               | -0.0832442  |\n","|    std                  | 1.01        |\n","|    value_loss           | 10.4        |\n","-----------------------------------------\n","Episode: 27\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 105        |\n","|    iterations           | 5          |\n","|    time_elapsed         | 97         |\n","|    total_timesteps      | 10240      |\n","| train/                  |            |\n","|    approx_kl            | 0.02257671 |\n","|    clip_fraction        | 0.225      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -41.4      |\n","|    explained_variance   | -0.00621   |\n","|    learning_rate        | 0.00025    |\n","|    loss                 | 6.96       |\n","|    n_updates            | 40         |\n","|    policy_gradient_loss | -0.0278    |\n","|    reward               | -1.1773021 |\n","|    std                  | 1.01       |\n","|    value_loss           | 19.7       |\n","----------------------------------------\n","Episode: 28\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 105        |\n","|    iterations           | 6          |\n","|    time_elapsed         | 116        |\n","|    total_timesteps      | 12288      |\n","| train/                  |            |\n","|    approx_kl            | 0.01684234 |\n","|    clip_fraction        | 0.172      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -41.4      |\n","|    explained_variance   | -0.0256    |\n","|    learning_rate        | 0.00025    |\n","|    loss                 | 6.69       |\n","|    n_updates            | 50         |\n","|    policy_gradient_loss | -0.0273    |\n","|    reward               | 2.9564948  |\n","|    std                  | 1.01       |\n","|    value_loss           | 12.3       |\n","----------------------------------------\n","Episode: 29\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 104         |\n","|    iterations           | 7           |\n","|    time_elapsed         | 136         |\n","|    total_timesteps      | 14336       |\n","| train/                  |             |\n","|    approx_kl            | 0.017210588 |\n","|    clip_fraction        | 0.195       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.5       |\n","|    explained_variance   | 0.0573      |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 4.09        |\n","|    n_updates            | 60          |\n","|    policy_gradient_loss | -0.0324     |\n","|    reward               | 0.89114684  |\n","|    std                  | 1.01        |\n","|    value_loss           | 11.3        |\n","-----------------------------------------\n","Episode: 30\n","day: 1888, episode: 30\n","begin_total_asset: 1000000.00\n","end_total_asset: 1862221.24\n","total_reward: 862221.24\n","total_cost: 302392.62\n","total_trades: 50593\n","Sharpe: 0.559\n","=================================\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 105         |\n","|    iterations           | 8           |\n","|    time_elapsed         | 155         |\n","|    total_timesteps      | 16384       |\n","| train/                  |             |\n","|    approx_kl            | 0.019556753 |\n","|    clip_fraction        | 0.21        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.5       |\n","|    explained_variance   | 0.0358      |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 5.41        |\n","|    n_updates            | 70          |\n","|    policy_gradient_loss | -0.0313     |\n","|    reward               | -0.99759495 |\n","|    std                  | 1.01        |\n","|    value_loss           | 13.1        |\n","-----------------------------------------\n","Episode: 31\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 105         |\n","|    iterations           | 9           |\n","|    time_elapsed         | 175         |\n","|    total_timesteps      | 18432       |\n","| train/                  |             |\n","|    approx_kl            | 0.020141788 |\n","|    clip_fraction        | 0.211       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.6       |\n","|    explained_variance   | 0.047       |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 4.13        |\n","|    n_updates            | 80          |\n","|    policy_gradient_loss | -0.03       |\n","|    reward               | 1.8544722   |\n","|    std                  | 1.02        |\n","|    value_loss           | 12.3        |\n","-----------------------------------------\n","Episode: 32\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 103         |\n","|    iterations           | 10          |\n","|    time_elapsed         | 197         |\n","|    total_timesteps      | 20480       |\n","| train/                  |             |\n","|    approx_kl            | 0.015093002 |\n","|    clip_fraction        | 0.162       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.7       |\n","|    explained_variance   | -0.0105     |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 6.68        |\n","|    n_updates            | 90          |\n","|    policy_gradient_loss | -0.0274     |\n","|    reward               | -1.4585371  |\n","|    std                  | 1.02        |\n","|    value_loss           | 15.2        |\n","-----------------------------------------\n","======ppo Validation from:  2022-07-06 to  2022-10-04\n","Episode: 1\n","ppo Sharpe Ratio:  -0.32861983431148556\n","======Best Model Retraining from:  2015-01-01 to  2022-10-04\n","======Trading from:  2022-10-04 to  2023-01-04\n","Used Model:  <stable_baselines3.ddpg.ddpg.DDPG object at 0x7a99d90fd150>\n","Episode: 1\n","============================================\n","turbulence_threshold:  182.0098142753525\n","======Model training from:  2015-01-01 to  2022-10-04\n","==============Model Training===========\n","======a2c Training========\n","{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n","Using cuda device\n","Logging to tensorboard_log//a2c/a2c_315_1\n","----------------------------------------\n","| time/                 |              |\n","|    fps                | 105          |\n","|    iterations         | 100          |\n","|    time_elapsed       | 4            |\n","|    total_timesteps    | 500          |\n","| train/                |              |\n","|    entropy_loss       | -41.2        |\n","|    explained_variance | 0.117        |\n","|    learning_rate      | 0.0007       |\n","|    n_updates          | 99           |\n","|    policy_loss        | -9.8         |\n","|    reward             | -0.020349983 |\n","|    std                | 1            |\n","|    value_loss         | 0.199        |\n","----------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 89         |\n","|    iterations         | 200        |\n","|    time_elapsed       | 11         |\n","|    total_timesteps    | 1000       |\n","| train/                |            |\n","|    entropy_loss       | -41.2      |\n","|    explained_variance | -0.37      |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 199        |\n","|    policy_loss        | -59.3      |\n","|    reward             | -2.0556371 |\n","|    std                | 1          |\n","|    value_loss         | 4.14       |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 94         |\n","|    iterations         | 300        |\n","|    time_elapsed       | 15         |\n","|    total_timesteps    | 1500       |\n","| train/                |            |\n","|    entropy_loss       | -41.1      |\n","|    explained_variance | 5.96e-08   |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 299        |\n","|    policy_loss        | 42.6       |\n","|    reward             | -0.3904846 |\n","|    std                | 1          |\n","|    value_loss         | 1.34       |\n","--------------------------------------\n","Episode: 1\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 97         |\n","|    iterations         | 400        |\n","|    time_elapsed       | 20         |\n","|    total_timesteps    | 2000       |\n","| train/                |            |\n","|    entropy_loss       | -41.1      |\n","|    explained_variance | -0.6       |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 399        |\n","|    policy_loss        | -95.5      |\n","|    reward             | -0.7000831 |\n","|    std                | 1          |\n","|    value_loss         | 6.05       |\n","--------------------------------------\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 91          |\n","|    iterations         | 500         |\n","|    time_elapsed       | 27          |\n","|    total_timesteps    | 2500        |\n","| train/                |             |\n","|    entropy_loss       | -41.1       |\n","|    explained_variance | -0.244      |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 499         |\n","|    policy_loss        | 61.6        |\n","|    reward             | -0.31457505 |\n","|    std                | 0.999       |\n","|    value_loss         | 2.73        |\n","---------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 94        |\n","|    iterations         | 600       |\n","|    time_elapsed       | 31        |\n","|    total_timesteps    | 3000      |\n","| train/                |           |\n","|    entropy_loss       | -41.1     |\n","|    explained_variance | -0.0347   |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 599       |\n","|    policy_loss        | -74.2     |\n","|    reward             | 2.6268854 |\n","|    std                | 0.999     |\n","|    value_loss         | 4.57      |\n","-------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 94        |\n","|    iterations         | 700       |\n","|    time_elapsed       | 37        |\n","|    total_timesteps    | 3500      |\n","| train/                |           |\n","|    entropy_loss       | -41.1     |\n","|    explained_variance | 0.00133   |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 699       |\n","|    policy_loss        | -97.6     |\n","|    reward             | 0.6762398 |\n","|    std                | 0.998     |\n","|    value_loss         | 8.46      |\n","-------------------------------------\n","Episode: 2\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 92         |\n","|    iterations         | 800        |\n","|    time_elapsed       | 43         |\n","|    total_timesteps    | 4000       |\n","| train/                |            |\n","|    entropy_loss       | -41.1      |\n","|    explained_variance | -0.116     |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 799        |\n","|    policy_loss        | 50.6       |\n","|    reward             | 0.37112746 |\n","|    std                | 1          |\n","|    value_loss         | 1.66       |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 94         |\n","|    iterations         | 900        |\n","|    time_elapsed       | 47         |\n","|    total_timesteps    | 4500       |\n","| train/                |            |\n","|    entropy_loss       | -41.2      |\n","|    explained_variance | -0.0836    |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 899        |\n","|    policy_loss        | -20.2      |\n","|    reward             | -1.9988924 |\n","|    std                | 1          |\n","|    value_loss         | 0.443      |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 92         |\n","|    iterations         | 1000       |\n","|    time_elapsed       | 53         |\n","|    total_timesteps    | 5000       |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | -0.103     |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 999        |\n","|    policy_loss        | 119        |\n","|    reward             | -1.7890024 |\n","|    std                | 1          |\n","|    value_loss         | 8.7        |\n","--------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 93        |\n","|    iterations         | 1100      |\n","|    time_elapsed       | 58        |\n","|    total_timesteps    | 5500      |\n","| train/                |           |\n","|    entropy_loss       | -41.2     |\n","|    explained_variance | 0.0816    |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 1099      |\n","|    policy_loss        | -66.5     |\n","|    reward             | 0.9455878 |\n","|    std                | 1         |\n","|    value_loss         | 3.37      |\n","-------------------------------------\n","Episode: 3\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 94          |\n","|    iterations         | 1200        |\n","|    time_elapsed       | 63          |\n","|    total_timesteps    | 6000        |\n","| train/                |             |\n","|    entropy_loss       | -41.2       |\n","|    explained_variance | 0.261       |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 1199        |\n","|    policy_loss        | -63.3       |\n","|    reward             | 0.058797315 |\n","|    std                | 1           |\n","|    value_loss         | 2.51        |\n","---------------------------------------\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 93          |\n","|    iterations         | 1300        |\n","|    time_elapsed       | 69          |\n","|    total_timesteps    | 6500        |\n","| train/                |             |\n","|    entropy_loss       | -41.2       |\n","|    explained_variance | 0.0276      |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 1299        |\n","|    policy_loss        | -6.22       |\n","|    reward             | -0.30078274 |\n","|    std                | 1           |\n","|    value_loss         | 0.125       |\n","---------------------------------------\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 94          |\n","|    iterations         | 1400        |\n","|    time_elapsed       | 74          |\n","|    total_timesteps    | 7000        |\n","| train/                |             |\n","|    entropy_loss       | -41.2       |\n","|    explained_variance | -7.65e-05   |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 1399        |\n","|    policy_loss        | 228         |\n","|    reward             | -0.93863815 |\n","|    std                | 1           |\n","|    value_loss         | 48.3        |\n","---------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 94       |\n","|    iterations         | 1500     |\n","|    time_elapsed       | 79       |\n","|    total_timesteps    | 7500     |\n","| train/                |          |\n","|    entropy_loss       | -41.2    |\n","|    explained_variance | 0.0348   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1499     |\n","|    policy_loss        | 60.1     |\n","|    reward             | 3.808156 |\n","|    std                | 1        |\n","|    value_loss         | 8.91     |\n","------------------------------------\n","Episode: 4\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 93       |\n","|    iterations         | 1600     |\n","|    time_elapsed       | 85       |\n","|    total_timesteps    | 8000     |\n","| train/                |          |\n","|    entropy_loss       | -41.3    |\n","|    explained_variance | 0.136    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1599     |\n","|    policy_loss        | -10.5    |\n","|    reward             | 0.753187 |\n","|    std                | 1.01     |\n","|    value_loss         | 2.32     |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 92        |\n","|    iterations         | 1700      |\n","|    time_elapsed       | 91        |\n","|    total_timesteps    | 8500      |\n","| train/                |           |\n","|    entropy_loss       | -41.4     |\n","|    explained_variance | 0         |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 1699      |\n","|    policy_loss        | -116      |\n","|    reward             | 0.5351417 |\n","|    std                | 1.01      |\n","|    value_loss         | 9.89      |\n","-------------------------------------\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 91          |\n","|    iterations         | 1800        |\n","|    time_elapsed       | 98          |\n","|    total_timesteps    | 9000        |\n","| train/                |             |\n","|    entropy_loss       | -41.4       |\n","|    explained_variance | -1.43       |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 1799        |\n","|    policy_loss        | -67.6       |\n","|    reward             | -0.43313166 |\n","|    std                | 1.01        |\n","|    value_loss         | 4.11        |\n","---------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 91        |\n","|    iterations         | 1900      |\n","|    time_elapsed       | 103       |\n","|    total_timesteps    | 9500      |\n","| train/                |           |\n","|    entropy_loss       | -41.4     |\n","|    explained_variance | 0.701     |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 1899      |\n","|    policy_loss        | 40.8      |\n","|    reward             | 1.0713376 |\n","|    std                | 1.01      |\n","|    value_loss         | 1.03      |\n","-------------------------------------\n","Episode: 5\n","day: 1951, episode: 5\n","begin_total_asset: 1000000.00\n","end_total_asset: 2062458.98\n","total_reward: 1062458.98\n","total_cost: 172977.35\n","total_trades: 40468\n","Sharpe: 0.564\n","=================================\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 92       |\n","|    iterations         | 2000     |\n","|    time_elapsed       | 108      |\n","|    total_timesteps    | 10000    |\n","| train/                |          |\n","|    entropy_loss       | -41.4    |\n","|    explained_variance | 3.58e-07 |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1999     |\n","|    policy_loss        | -27.7    |\n","|    reward             | 1.405498 |\n","|    std                | 1.01     |\n","|    value_loss         | 1.23     |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 91       |\n","|    iterations         | 2100     |\n","|    time_elapsed       | 114      |\n","|    total_timesteps    | 10500    |\n","| train/                |          |\n","|    entropy_loss       | -41.4    |\n","|    explained_variance | -0.111   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2099     |\n","|    policy_loss        | 121      |\n","|    reward             | 2.183242 |\n","|    std                | 1.01     |\n","|    value_loss         | 17.4     |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 92        |\n","|    iterations         | 2200      |\n","|    time_elapsed       | 119       |\n","|    total_timesteps    | 11000     |\n","| train/                |           |\n","|    entropy_loss       | -41.4     |\n","|    explained_variance | -0.0275   |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 2199      |\n","|    policy_loss        | 61.8      |\n","|    reward             | 0.6095073 |\n","|    std                | 1.01      |\n","|    value_loss         | 3.93      |\n","-------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 92        |\n","|    iterations         | 2300      |\n","|    time_elapsed       | 124       |\n","|    total_timesteps    | 11500     |\n","| train/                |           |\n","|    entropy_loss       | -41.3     |\n","|    explained_variance | -0.0136   |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 2299      |\n","|    policy_loss        | -198      |\n","|    reward             | -6.016678 |\n","|    std                | 1.01      |\n","|    value_loss         | 34.7      |\n","-------------------------------------\n","Episode: 6\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 92        |\n","|    iterations         | 2400      |\n","|    time_elapsed       | 130       |\n","|    total_timesteps    | 12000     |\n","| train/                |           |\n","|    entropy_loss       | -41.4     |\n","|    explained_variance | 6.56e-07  |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 2399      |\n","|    policy_loss        | 75.9      |\n","|    reward             | 0.9459038 |\n","|    std                | 1.01      |\n","|    value_loss         | 6.4       |\n","-------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 92         |\n","|    iterations         | 2500       |\n","|    time_elapsed       | 134        |\n","|    total_timesteps    | 12500      |\n","| train/                |            |\n","|    entropy_loss       | -41.4      |\n","|    explained_variance | 0          |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 2499       |\n","|    policy_loss        | -1.18      |\n","|    reward             | -1.1991932 |\n","|    std                | 1.01       |\n","|    value_loss         | 5.7        |\n","--------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 91        |\n","|    iterations         | 2600      |\n","|    time_elapsed       | 141       |\n","|    total_timesteps    | 13000     |\n","| train/                |           |\n","|    entropy_loss       | -41.4     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 2599      |\n","|    policy_loss        | 66.6      |\n","|    reward             | 0.445108  |\n","|    std                | 1.01      |\n","|    value_loss         | 10.9      |\n","-------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 92       |\n","|    iterations         | 2700     |\n","|    time_elapsed       | 146      |\n","|    total_timesteps    | 13500    |\n","| train/                |          |\n","|    entropy_loss       | -41.4    |\n","|    explained_variance | -0.00762 |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2699     |\n","|    policy_loss        | 377      |\n","|    reward             | 0.986165 |\n","|    std                | 1.01     |\n","|    value_loss         | 96       |\n","------------------------------------\n","Episode: 7\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 92        |\n","|    iterations         | 2800      |\n","|    time_elapsed       | 150       |\n","|    total_timesteps    | 14000     |\n","| train/                |           |\n","|    entropy_loss       | -41.4     |\n","|    explained_variance | -1.18     |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 2799      |\n","|    policy_loss        | -6.83     |\n","|    reward             | 0.1573045 |\n","|    std                | 1.01      |\n","|    value_loss         | 0.55      |\n","-------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 92        |\n","|    iterations         | 2900      |\n","|    time_elapsed       | 157       |\n","|    total_timesteps    | 14500     |\n","| train/                |           |\n","|    entropy_loss       | -41.4     |\n","|    explained_variance | -0.168    |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 2899      |\n","|    policy_loss        | -68.4     |\n","|    reward             | 2.1108315 |\n","|    std                | 1.01      |\n","|    value_loss         | 4.22      |\n","-------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 92       |\n","|    iterations         | 3000     |\n","|    time_elapsed       | 161      |\n","|    total_timesteps    | 15000    |\n","| train/                |          |\n","|    entropy_loss       | -41.5    |\n","|    explained_variance | 0.0151   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2999     |\n","|    policy_loss        | 258      |\n","|    reward             | 2.785006 |\n","|    std                | 1.01     |\n","|    value_loss         | 56.7     |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 93        |\n","|    iterations         | 3100      |\n","|    time_elapsed       | 166       |\n","|    total_timesteps    | 15500     |\n","| train/                |           |\n","|    entropy_loss       | -41.5     |\n","|    explained_variance | -0.0098   |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 3099      |\n","|    policy_loss        | 176       |\n","|    reward             | 0.9816749 |\n","|    std                | 1.01      |\n","|    value_loss         | 67.3      |\n","-------------------------------------\n","Episode: 8\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 92        |\n","|    iterations         | 3200      |\n","|    time_elapsed       | 173       |\n","|    total_timesteps    | 16000     |\n","| train/                |           |\n","|    entropy_loss       | -41.5     |\n","|    explained_variance | 0.176     |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 3199      |\n","|    policy_loss        | 14.2      |\n","|    reward             | 1.1400979 |\n","|    std                | 1.01      |\n","|    value_loss         | 1.08      |\n","-------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 92        |\n","|    iterations         | 3300      |\n","|    time_elapsed       | 177       |\n","|    total_timesteps    | 16500     |\n","| train/                |           |\n","|    entropy_loss       | -41.6     |\n","|    explained_variance | 0.00397   |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 3299      |\n","|    policy_loss        | -207      |\n","|    reward             | 1.3979175 |\n","|    std                | 1.01      |\n","|    value_loss         | 35.2      |\n","-------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 92         |\n","|    iterations         | 3400       |\n","|    time_elapsed       | 182        |\n","|    total_timesteps    | 17000      |\n","| train/                |            |\n","|    entropy_loss       | -41.5      |\n","|    explained_variance | -0.109     |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3399       |\n","|    policy_loss        | -20.2      |\n","|    reward             | 0.69421864 |\n","|    std                | 1.01       |\n","|    value_loss         | 4.52       |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 92         |\n","|    iterations         | 3500       |\n","|    time_elapsed       | 188        |\n","|    total_timesteps    | 17500      |\n","| train/                |            |\n","|    entropy_loss       | -41.6      |\n","|    explained_variance | -0.0153    |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3499       |\n","|    policy_loss        | -86.6      |\n","|    reward             | -2.3719168 |\n","|    std                | 1.01       |\n","|    value_loss         | 52.5       |\n","--------------------------------------\n","Episode: 9\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 93         |\n","|    iterations         | 3600       |\n","|    time_elapsed       | 193        |\n","|    total_timesteps    | 18000      |\n","| train/                |            |\n","|    entropy_loss       | -41.6      |\n","|    explained_variance | -0.185     |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3599       |\n","|    policy_loss        | -55.6      |\n","|    reward             | 0.28035203 |\n","|    std                | 1.02       |\n","|    value_loss         | 2.73       |\n","--------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 92        |\n","|    iterations         | 3700      |\n","|    time_elapsed       | 199       |\n","|    total_timesteps    | 18500     |\n","| train/                |           |\n","|    entropy_loss       | -41.6     |\n","|    explained_variance | 0.254     |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 3699      |\n","|    policy_loss        | -66.8     |\n","|    reward             | 0.2857878 |\n","|    std                | 1.02      |\n","|    value_loss         | 3.3       |\n","-------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 92         |\n","|    iterations         | 3800       |\n","|    time_elapsed       | 206        |\n","|    total_timesteps    | 19000      |\n","| train/                |            |\n","|    entropy_loss       | -41.6      |\n","|    explained_variance | 0.0357     |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3799       |\n","|    policy_loss        | 117        |\n","|    reward             | -1.5610476 |\n","|    std                | 1.02       |\n","|    value_loss         | 8.7        |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 92         |\n","|    iterations         | 3900       |\n","|    time_elapsed       | 210        |\n","|    total_timesteps    | 19500      |\n","| train/                |            |\n","|    entropy_loss       | -41.6      |\n","|    explained_variance | 0.0288     |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3899       |\n","|    policy_loss        | -231       |\n","|    reward             | 0.11030267 |\n","|    std                | 1.02       |\n","|    value_loss         | 36         |\n","--------------------------------------\n","Episode: 10\n","day: 1951, episode: 10\n","begin_total_asset: 1000000.00\n","end_total_asset: 2724409.95\n","total_reward: 1724409.95\n","total_cost: 99428.79\n","total_trades: 39554\n","Sharpe: 0.765\n","=================================\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 91        |\n","|    iterations         | 4000      |\n","|    time_elapsed       | 217       |\n","|    total_timesteps    | 20000     |\n","| train/                |           |\n","|    entropy_loss       | -41.7     |\n","|    explained_variance | 0.159     |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 3999      |\n","|    policy_loss        | 20.2      |\n","|    reward             | 0.4277128 |\n","|    std                | 1.02      |\n","|    value_loss         | 0.89      |\n","-------------------------------------\n","======a2c Validation from:  2022-10-04 to  2023-01-04\n","Episode: 1\n","a2c Sharpe Ratio:  0.2991371079249242\n","======ddpg Training========\n","{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n","Using cuda device\n","Logging to tensorboard_log//ddpg/ddpg_315_1\n","Episode: 12\n","Episode: 13\n","Episode: 14\n","Episode: 15\n","day: 1951, episode: 15\n","begin_total_asset: 1000000.00\n","end_total_asset: 2328757.26\n","total_reward: 1328757.26\n","total_cost: 998.98\n","total_trades: 21461\n","Sharpe: 0.683\n","=================================\n","----------------------------------\n","| time/              |           |\n","|    episodes        | 4         |\n","|    fps             | 58        |\n","|    time_elapsed    | 132       |\n","|    total_timesteps | 7808      |\n","| train/             |           |\n","|    actor_loss      | -36.6     |\n","|    critic_loss     | 165       |\n","|    learning_rate   | 0.0005    |\n","|    n_updates       | 7707      |\n","|    reward          | 5.6458592 |\n","----------------------------------\n","Episode: 16\n","Episode: 17\n","Episode: 18\n","Episode: 19\n","----------------------------------\n","| time/              |           |\n","|    episodes        | 8         |\n","|    fps             | 58        |\n","|    time_elapsed    | 267       |\n","|    total_timesteps | 15616     |\n","| train/             |           |\n","|    actor_loss      | -9.52     |\n","|    critic_loss     | 5.45      |\n","|    learning_rate   | 0.0005    |\n","|    n_updates       | 15515     |\n","|    reward          | 5.6458592 |\n","----------------------------------\n","Episode: 20\n","day: 1951, episode: 20\n","begin_total_asset: 1000000.00\n","end_total_asset: 2328757.26\n","total_reward: 1328757.26\n","total_cost: 998.98\n","total_trades: 21461\n","Sharpe: 0.683\n","=================================\n","Episode: 21\n","======ddpg Validation from:  2022-10-04 to  2023-01-04\n","Episode: 1\n","ddpg Sharpe Ratio:  0.22531199779636082\n","======ppo Training========\n","{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n","Using cuda device\n","Logging to tensorboard_log//ppo/ppo_315_1\n","Episode: 23\n","-----------------------------------\n","| time/              |            |\n","|    fps             | 100        |\n","|    iterations      | 1          |\n","|    time_elapsed    | 20         |\n","|    total_timesteps | 2048       |\n","| train/             |            |\n","|    reward          | 0.20543665 |\n","-----------------------------------\n","Episode: 24\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 102         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 40          |\n","|    total_timesteps      | 4096        |\n","| train/                  |             |\n","|    approx_kl            | 0.015113389 |\n","|    clip_fraction        | 0.225       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.2       |\n","|    explained_variance   | 0.00266     |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 7.61        |\n","|    n_updates            | 10          |\n","|    policy_gradient_loss | -0.0343     |\n","|    reward               | 1.0980781   |\n","|    std                  | 1           |\n","|    value_loss           | 13.9        |\n","-----------------------------------------\n","Episode: 25\n","day: 1951, episode: 25\n","begin_total_asset: 1000000.00\n","end_total_asset: 1604062.12\n","total_reward: 604062.12\n","total_cost: 319007.16\n","total_trades: 52538\n","Sharpe: 0.434\n","=================================\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 103         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 59          |\n","|    total_timesteps      | 6144        |\n","| train/                  |             |\n","|    approx_kl            | 0.015050086 |\n","|    clip_fraction        | 0.153       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.2       |\n","|    explained_variance   | -0.0177     |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 14.8        |\n","|    n_updates            | 20          |\n","|    policy_gradient_loss | -0.0303     |\n","|    reward               | 1.0671867   |\n","|    std                  | 1           |\n","|    value_loss           | 24          |\n","-----------------------------------------\n","Episode: 26\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 99          |\n","|    iterations           | 4           |\n","|    time_elapsed         | 82          |\n","|    total_timesteps      | 8192        |\n","| train/                  |             |\n","|    approx_kl            | 0.015170276 |\n","|    clip_fraction        | 0.195       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.3       |\n","|    explained_variance   | -0.0031     |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 6.45        |\n","|    n_updates            | 30          |\n","|    policy_gradient_loss | -0.0355     |\n","|    reward               | 0.1307324   |\n","|    std                  | 1.01        |\n","|    value_loss           | 14.9        |\n","-----------------------------------------\n","Episode: 27\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 99          |\n","|    iterations           | 5           |\n","|    time_elapsed         | 102         |\n","|    total_timesteps      | 10240       |\n","| train/                  |             |\n","|    approx_kl            | 0.017320618 |\n","|    clip_fraction        | 0.19        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.4       |\n","|    explained_variance   | 0.0115      |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 12.4        |\n","|    n_updates            | 40          |\n","|    policy_gradient_loss | -0.0332     |\n","|    reward               | -0.30871418 |\n","|    std                  | 1.01        |\n","|    value_loss           | 21.9        |\n","-----------------------------------------\n","Episode: 28\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 100         |\n","|    iterations           | 6           |\n","|    time_elapsed         | 121         |\n","|    total_timesteps      | 12288       |\n","| train/                  |             |\n","|    approx_kl            | 0.018613111 |\n","|    clip_fraction        | 0.186       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.4       |\n","|    explained_variance   | 0.0112      |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 9.23        |\n","|    n_updates            | 50          |\n","|    policy_gradient_loss | -0.0286     |\n","|    reward               | -0.54347116 |\n","|    std                  | 1.01        |\n","|    value_loss           | 21.8        |\n","-----------------------------------------\n","Episode: 29\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 100        |\n","|    iterations           | 7          |\n","|    time_elapsed         | 143        |\n","|    total_timesteps      | 14336      |\n","| train/                  |            |\n","|    approx_kl            | 0.01837608 |\n","|    clip_fraction        | 0.183      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -41.4      |\n","|    explained_variance   | 0.0157     |\n","|    learning_rate        | 0.00025    |\n","|    loss                 | 8.05       |\n","|    n_updates            | 60         |\n","|    policy_gradient_loss | -0.0303    |\n","|    reward               | 0.28002203 |\n","|    std                  | 1.01       |\n","|    value_loss           | 14.2       |\n","----------------------------------------\n","Episode: 30\n","day: 1951, episode: 30\n","begin_total_asset: 1000000.00\n","end_total_asset: 1962798.36\n","total_reward: 962798.36\n","total_cost: 316233.28\n","total_trades: 52146\n","Sharpe: 0.570\n","=================================\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 100          |\n","|    iterations           | 8            |\n","|    time_elapsed         | 162          |\n","|    total_timesteps      | 16384        |\n","| train/                  |              |\n","|    approx_kl            | 0.0139903445 |\n","|    clip_fraction        | 0.157        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -41.4        |\n","|    explained_variance   | 0.0386       |\n","|    learning_rate        | 0.00025      |\n","|    loss                 | 5.09         |\n","|    n_updates            | 70           |\n","|    policy_gradient_loss | -0.0281      |\n","|    reward               | 0.41094813   |\n","|    std                  | 1.01         |\n","|    value_loss           | 14.8         |\n","------------------------------------------\n","Episode: 31\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 101         |\n","|    iterations           | 9           |\n","|    time_elapsed         | 182         |\n","|    total_timesteps      | 18432       |\n","| train/                  |             |\n","|    approx_kl            | 0.017800266 |\n","|    clip_fraction        | 0.175       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.4       |\n","|    explained_variance   | 0.000537    |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 7.16        |\n","|    n_updates            | 80          |\n","|    policy_gradient_loss | -0.0246     |\n","|    reward               | 0.7494718   |\n","|    std                  | 1.01        |\n","|    value_loss           | 22.8        |\n","-----------------------------------------\n","Episode: 32\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 99          |\n","|    iterations           | 10          |\n","|    time_elapsed         | 205         |\n","|    total_timesteps      | 20480       |\n","| train/                  |             |\n","|    approx_kl            | 0.016092123 |\n","|    clip_fraction        | 0.168       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.5       |\n","|    explained_variance   | 0.0266      |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 9.26        |\n","|    n_updates            | 90          |\n","|    policy_gradient_loss | -0.0272     |\n","|    reward               | -3.4718533  |\n","|    std                  | 1.01        |\n","|    value_loss           | 26.3        |\n","-----------------------------------------\n","======ppo Validation from:  2022-10-04 to  2023-01-04\n","Episode: 1\n","ppo Sharpe Ratio:  0.2553178258460387\n","======Best Model Retraining from:  2015-01-01 to  2023-01-04\n","======Trading from:  2023-01-04 to  2023-04-05\n","Used Model:  <stable_baselines3.a2c.a2c.A2C object at 0x7a99d3581f90>\n","Episode: 1\n","============================================\n","turbulence_threshold:  182.0098142753525\n","======Model training from:  2015-01-01 to  2023-01-04\n","==============Model Training===========\n","======a2c Training========\n","{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n","Using cuda device\n","Logging to tensorboard_log//a2c/a2c_378_1\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 104       |\n","|    iterations         | 100       |\n","|    time_elapsed       | 4         |\n","|    total_timesteps    | 500       |\n","| train/                |           |\n","|    entropy_loss       | -41.2     |\n","|    explained_variance | -1.35     |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 99        |\n","|    policy_loss        | -3.23     |\n","|    reward             | 0.2876538 |\n","|    std                | 1         |\n","|    value_loss         | 0.629     |\n","-------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 86         |\n","|    iterations         | 200        |\n","|    time_elapsed       | 11         |\n","|    total_timesteps    | 1000       |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | 0          |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 199        |\n","|    policy_loss        | -95.1      |\n","|    reward             | -2.7758799 |\n","|    std                | 1          |\n","|    value_loss         | 5.26       |\n","--------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 91        |\n","|    iterations         | 300       |\n","|    time_elapsed       | 16        |\n","|    total_timesteps    | 1500      |\n","| train/                |           |\n","|    entropy_loss       | -41.2     |\n","|    explained_variance | 0         |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 299       |\n","|    policy_loss        | -108      |\n","|    reward             | 1.4989429 |\n","|    std                | 1         |\n","|    value_loss         | 7.24      |\n","-------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 94       |\n","|    iterations         | 400      |\n","|    time_elapsed       | 21       |\n","|    total_timesteps    | 2000     |\n","| train/                |          |\n","|    entropy_loss       | -41.2    |\n","|    explained_variance | 0.0143   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 399      |\n","|    policy_loss        | -0.729   |\n","|    reward             | 5.867769 |\n","|    std                | 1        |\n","|    value_loss         | 29.8     |\n","------------------------------------\n","Episode: 1\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 89         |\n","|    iterations         | 500        |\n","|    time_elapsed       | 28         |\n","|    total_timesteps    | 2500       |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | 0.039      |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 499        |\n","|    policy_loss        | 40.2       |\n","|    reward             | 0.67897093 |\n","|    std                | 1.01       |\n","|    value_loss         | 1.09       |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 91         |\n","|    iterations         | 600        |\n","|    time_elapsed       | 32         |\n","|    total_timesteps    | 3000       |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | 0.026      |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 599        |\n","|    policy_loss        | -131       |\n","|    reward             | -1.3553233 |\n","|    std                | 1.01       |\n","|    value_loss         | 17.7       |\n","--------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 91       |\n","|    iterations         | 700      |\n","|    time_elapsed       | 38       |\n","|    total_timesteps    | 3500     |\n","| train/                |          |\n","|    entropy_loss       | -41.3    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 699      |\n","|    policy_loss        | 5.14     |\n","|    reward             | 4.432446 |\n","|    std                | 1        |\n","|    value_loss         | 2.94     |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 90        |\n","|    iterations         | 800       |\n","|    time_elapsed       | 44        |\n","|    total_timesteps    | 4000      |\n","| train/                |           |\n","|    entropy_loss       | -41.3     |\n","|    explained_variance | 0.00157   |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 799       |\n","|    policy_loss        | 396       |\n","|    reward             | 1.5164325 |\n","|    std                | 1         |\n","|    value_loss         | 96.7      |\n","-------------------------------------\n","Episode: 2\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 91         |\n","|    iterations         | 900        |\n","|    time_elapsed       | 49         |\n","|    total_timesteps    | 4500       |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | 0.334      |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 899        |\n","|    policy_loss        | -47.3      |\n","|    reward             | 0.94426304 |\n","|    std                | 1          |\n","|    value_loss         | 1.4        |\n","--------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 89        |\n","|    iterations         | 1000      |\n","|    time_elapsed       | 55        |\n","|    total_timesteps    | 5000      |\n","| train/                |           |\n","|    entropy_loss       | -41.2     |\n","|    explained_variance | 0.111     |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 999       |\n","|    policy_loss        | -7.59     |\n","|    reward             | 1.9127659 |\n","|    std                | 1         |\n","|    value_loss         | 1.19      |\n","-------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 90        |\n","|    iterations         | 1100      |\n","|    time_elapsed       | 60        |\n","|    total_timesteps    | 5500      |\n","| train/                |           |\n","|    entropy_loss       | -41.3     |\n","|    explained_variance | 0         |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 1099      |\n","|    policy_loss        | -78       |\n","|    reward             | 2.8074389 |\n","|    std                | 1.01      |\n","|    value_loss         | 4.51      |\n","-------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 91        |\n","|    iterations         | 1200      |\n","|    time_elapsed       | 65        |\n","|    total_timesteps    | 6000      |\n","| train/                |           |\n","|    entropy_loss       | -41.2     |\n","|    explained_variance | -0.262    |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 1199      |\n","|    policy_loss        | 156       |\n","|    reward             | 5.2750525 |\n","|    std                | 1         |\n","|    value_loss         | 18.8      |\n","-------------------------------------\n","Episode: 3\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 89         |\n","|    iterations         | 1300       |\n","|    time_elapsed       | 72         |\n","|    total_timesteps    | 6500       |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | 0.115      |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 1299       |\n","|    policy_loss        | -7.43      |\n","|    reward             | -0.9956342 |\n","|    std                | 1.01       |\n","|    value_loss         | 0.369      |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 90         |\n","|    iterations         | 1400       |\n","|    time_elapsed       | 77         |\n","|    total_timesteps    | 7000       |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | 0.0216     |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 1399       |\n","|    policy_loss        | -217       |\n","|    reward             | -0.2626305 |\n","|    std                | 1.01       |\n","|    value_loss         | 29.3       |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 91         |\n","|    iterations         | 1500       |\n","|    time_elapsed       | 82         |\n","|    total_timesteps    | 7500       |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | -1.19e-07  |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 1499       |\n","|    policy_loss        | -24.6      |\n","|    reward             | -1.4841481 |\n","|    std                | 1.01       |\n","|    value_loss         | 1.17       |\n","--------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 89       |\n","|    iterations         | 1600     |\n","|    time_elapsed       | 89       |\n","|    total_timesteps    | 8000     |\n","| train/                |          |\n","|    entropy_loss       | -41.3    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1599     |\n","|    policy_loss        | -132     |\n","|    reward             | -2.04107 |\n","|    std                | 1.01     |\n","|    value_loss         | 13.3     |\n","------------------------------------\n","Episode: 4\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 89        |\n","|    iterations         | 1700      |\n","|    time_elapsed       | 95        |\n","|    total_timesteps    | 8500      |\n","| train/                |           |\n","|    entropy_loss       | -41.4     |\n","|    explained_variance | -3.12e-05 |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 1699      |\n","|    policy_loss        | 11.2      |\n","|    reward             | 0.8193115 |\n","|    std                | 1.01      |\n","|    value_loss         | 0.477     |\n","-------------------------------------\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 88          |\n","|    iterations         | 1800        |\n","|    time_elapsed       | 101         |\n","|    total_timesteps    | 9000        |\n","| train/                |             |\n","|    entropy_loss       | -41.4       |\n","|    explained_variance | -0.185      |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 1799        |\n","|    policy_loss        | -14.9       |\n","|    reward             | -0.07394931 |\n","|    std                | 1.01        |\n","|    value_loss         | 0.608       |\n","---------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 89        |\n","|    iterations         | 1900      |\n","|    time_elapsed       | 106       |\n","|    total_timesteps    | 9500      |\n","| train/                |           |\n","|    entropy_loss       | -41.4     |\n","|    explained_variance | -0.172    |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 1899      |\n","|    policy_loss        | 140       |\n","|    reward             | 3.2679255 |\n","|    std                | 1.01      |\n","|    value_loss         | 16        |\n","-------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 89        |\n","|    iterations         | 2000      |\n","|    time_elapsed       | 112       |\n","|    total_timesteps    | 10000     |\n","| train/                |           |\n","|    entropy_loss       | -41.4     |\n","|    explained_variance | 0.00297   |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 1999      |\n","|    policy_loss        | 219       |\n","|    reward             | -0.536543 |\n","|    std                | 1.01      |\n","|    value_loss         | 24.8      |\n","-------------------------------------\n","Episode: 5\n","day: 2014, episode: 5\n","begin_total_asset: 1000000.00\n","end_total_asset: 2580913.39\n","total_reward: 1580913.39\n","total_cost: 54697.44\n","total_trades: 35790\n","Sharpe: 0.652\n","=================================\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 89         |\n","|    iterations         | 2100       |\n","|    time_elapsed       | 117        |\n","|    total_timesteps    | 10500      |\n","| train/                |            |\n","|    entropy_loss       | -41.5      |\n","|    explained_variance | 0.331      |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 2099       |\n","|    policy_loss        | 17.4       |\n","|    reward             | -2.3375971 |\n","|    std                | 1.01       |\n","|    value_loss         | 0.223      |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 89         |\n","|    iterations         | 2200       |\n","|    time_elapsed       | 122        |\n","|    total_timesteps    | 11000      |\n","| train/                |            |\n","|    entropy_loss       | -41.6      |\n","|    explained_variance | 1.19e-07   |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 2199       |\n","|    policy_loss        | 36         |\n","|    reward             | 0.83094347 |\n","|    std                | 1.02       |\n","|    value_loss         | 0.978      |\n","--------------------------------------\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 88          |\n","|    iterations         | 2300        |\n","|    time_elapsed       | 129         |\n","|    total_timesteps    | 11500       |\n","| train/                |             |\n","|    entropy_loss       | -41.7       |\n","|    explained_variance | 5.96e-08    |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 2299        |\n","|    policy_loss        | 68.5        |\n","|    reward             | -0.18319829 |\n","|    std                | 1.02        |\n","|    value_loss         | 3.32        |\n","---------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 89        |\n","|    iterations         | 2400      |\n","|    time_elapsed       | 134       |\n","|    total_timesteps    | 12000     |\n","| train/                |           |\n","|    entropy_loss       | -41.7     |\n","|    explained_variance | -0.00151  |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 2399      |\n","|    policy_loss        | -34.9     |\n","|    reward             | 2.4796312 |\n","|    std                | 1.02      |\n","|    value_loss         | 3.51      |\n","-------------------------------------\n","Episode: 6\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 89          |\n","|    iterations         | 2500        |\n","|    time_elapsed       | 138         |\n","|    total_timesteps    | 12500       |\n","| train/                |             |\n","|    entropy_loss       | -41.7       |\n","|    explained_variance | 0           |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 2499        |\n","|    policy_loss        | 25.6        |\n","|    reward             | -0.06309912 |\n","|    std                | 1.02        |\n","|    value_loss         | 0.377       |\n","---------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 89        |\n","|    iterations         | 2600      |\n","|    time_elapsed       | 145       |\n","|    total_timesteps    | 13000     |\n","| train/                |           |\n","|    entropy_loss       | -41.7     |\n","|    explained_variance | 5.96e-08  |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 2599      |\n","|    policy_loss        | 6.75      |\n","|    reward             | 2.5613396 |\n","|    std                | 1.02      |\n","|    value_loss         | 0.202     |\n","-------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 89        |\n","|    iterations         | 2700      |\n","|    time_elapsed       | 150       |\n","|    total_timesteps    | 13500     |\n","| train/                |           |\n","|    entropy_loss       | -41.7     |\n","|    explained_variance | -0.00601  |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 2699      |\n","|    policy_loss        | -71.6     |\n","|    reward             | 2.0464318 |\n","|    std                | 1.02      |\n","|    value_loss         | 5.11      |\n","-------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 89         |\n","|    iterations         | 2800       |\n","|    time_elapsed       | 155        |\n","|    total_timesteps    | 14000      |\n","| train/                |            |\n","|    entropy_loss       | -41.7      |\n","|    explained_variance | 0          |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 2799       |\n","|    policy_loss        | 187        |\n","|    reward             | -2.1169786 |\n","|    std                | 1.02       |\n","|    value_loss         | 25.5       |\n","--------------------------------------\n","Episode: 7\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 89          |\n","|    iterations         | 2900        |\n","|    time_elapsed       | 161         |\n","|    total_timesteps    | 14500       |\n","| train/                |             |\n","|    entropy_loss       | -41.7       |\n","|    explained_variance | 0.0102      |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 2899        |\n","|    policy_loss        | 31.7        |\n","|    reward             | -0.03889921 |\n","|    std                | 1.02        |\n","|    value_loss         | 0.912       |\n","---------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 90         |\n","|    iterations         | 3000       |\n","|    time_elapsed       | 166        |\n","|    total_timesteps    | 15000      |\n","| train/                |            |\n","|    entropy_loss       | -41.8      |\n","|    explained_variance | 0.0154     |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 2999       |\n","|    policy_loss        | 4.82       |\n","|    reward             | 0.51401454 |\n","|    std                | 1.02       |\n","|    value_loss         | 0.261      |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 89         |\n","|    iterations         | 3100       |\n","|    time_elapsed       | 172        |\n","|    total_timesteps    | 15500      |\n","| train/                |            |\n","|    entropy_loss       | -41.7      |\n","|    explained_variance | 0          |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3099       |\n","|    policy_loss        | -53.3      |\n","|    reward             | -1.0514143 |\n","|    std                | 1.02       |\n","|    value_loss         | 4.05       |\n","--------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 89        |\n","|    iterations         | 3200      |\n","|    time_elapsed       | 177       |\n","|    total_timesteps    | 16000     |\n","| train/                |           |\n","|    entropy_loss       | -41.7     |\n","|    explained_variance | 5.96e-08  |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 3199      |\n","|    policy_loss        | 29.1      |\n","|    reward             | 0.9169228 |\n","|    std                | 1.02      |\n","|    value_loss         | 1.5       |\n","-------------------------------------\n","Episode: 8\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 90          |\n","|    iterations         | 3300        |\n","|    time_elapsed       | 182         |\n","|    total_timesteps    | 16500       |\n","| train/                |             |\n","|    entropy_loss       | -41.7       |\n","|    explained_variance | 1.79e-07    |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 3299        |\n","|    policy_loss        | 12.6        |\n","|    reward             | -0.83818614 |\n","|    std                | 1.02        |\n","|    value_loss         | 2.68        |\n","---------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 89         |\n","|    iterations         | 3400       |\n","|    time_elapsed       | 189        |\n","|    total_timesteps    | 17000      |\n","| train/                |            |\n","|    entropy_loss       | -41.7      |\n","|    explained_variance | 0          |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3399       |\n","|    policy_loss        | -126       |\n","|    reward             | 0.34017107 |\n","|    std                | 1.02       |\n","|    value_loss         | 11.7       |\n","--------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 90        |\n","|    iterations         | 3500      |\n","|    time_elapsed       | 194       |\n","|    total_timesteps    | 17500     |\n","| train/                |           |\n","|    entropy_loss       | -41.7     |\n","|    explained_variance | 5.96e-08  |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 3499      |\n","|    policy_loss        | -124      |\n","|    reward             | -3.841101 |\n","|    std                | 1.02      |\n","|    value_loss         | 8.1       |\n","-------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 90         |\n","|    iterations         | 3600       |\n","|    time_elapsed       | 199        |\n","|    total_timesteps    | 18000      |\n","| train/                |            |\n","|    entropy_loss       | -41.7      |\n","|    explained_variance | 1.79e-07   |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3599       |\n","|    policy_loss        | -303       |\n","|    reward             | 0.39355367 |\n","|    std                | 1.02       |\n","|    value_loss         | 84.7       |\n","--------------------------------------\n","Episode: 9\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 88          |\n","|    iterations         | 3700        |\n","|    time_elapsed       | 207         |\n","|    total_timesteps    | 18500       |\n","| train/                |             |\n","|    entropy_loss       | -41.7       |\n","|    explained_variance | 0           |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 3699        |\n","|    policy_loss        | 59.4        |\n","|    reward             | -0.41645157 |\n","|    std                | 1.02        |\n","|    value_loss         | 2.66        |\n","---------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 89       |\n","|    iterations         | 3800     |\n","|    time_elapsed       | 212      |\n","|    total_timesteps    | 19000    |\n","| train/                |          |\n","|    entropy_loss       | -41.7    |\n","|    explained_variance | 0.0435   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 3799     |\n","|    policy_loss        | 43.2     |\n","|    reward             | 0.263648 |\n","|    std                | 1.02     |\n","|    value_loss         | 2.34     |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 89        |\n","|    iterations         | 3900      |\n","|    time_elapsed       | 217       |\n","|    total_timesteps    | 19500     |\n","| train/                |           |\n","|    entropy_loss       | -41.7     |\n","|    explained_variance | 0         |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 3899      |\n","|    policy_loss        | 207       |\n","|    reward             | 6.5561156 |\n","|    std                | 1.02      |\n","|    value_loss         | 35.4      |\n","-------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 89         |\n","|    iterations         | 4000       |\n","|    time_elapsed       | 224        |\n","|    total_timesteps    | 20000      |\n","| train/                |            |\n","|    entropy_loss       | -41.7      |\n","|    explained_variance | 0          |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3999       |\n","|    policy_loss        | 42.7       |\n","|    reward             | -0.8416757 |\n","|    std                | 1.02       |\n","|    value_loss         | 2.02       |\n","--------------------------------------\n","======a2c Validation from:  2023-01-04 to  2023-04-05\n","Episode: 1\n","a2c Sharpe Ratio:  0.03447555989785029\n","======ddpg Training========\n","{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n","Using cuda device\n","Logging to tensorboard_log//ddpg/ddpg_378_1\n","Episode: 11\n","Episode: 12\n","Episode: 13\n","Episode: 14\n","------------------------------------\n","| time/              |             |\n","|    episodes        | 4           |\n","|    fps             | 58          |\n","|    time_elapsed    | 137         |\n","|    total_timesteps | 8060        |\n","| train/             |             |\n","|    actor_loss      | -90.1       |\n","|    critic_loss     | 177         |\n","|    learning_rate   | 0.0005      |\n","|    n_updates       | 7959        |\n","|    reward          | -0.42198265 |\n","------------------------------------\n","Episode: 15\n","day: 2014, episode: 15\n","begin_total_asset: 1000000.00\n","end_total_asset: 2401323.41\n","total_reward: 1401323.41\n","total_cost: 999.00\n","total_trades: 32212\n","Sharpe: 0.649\n","=================================\n","Episode: 16\n","Episode: 17\n","Episode: 18\n","------------------------------------\n","| time/              |             |\n","|    episodes        | 8           |\n","|    fps             | 58          |\n","|    time_elapsed    | 276         |\n","|    total_timesteps | 16120       |\n","| train/             |             |\n","|    actor_loss      | -108        |\n","|    critic_loss     | 7.1         |\n","|    learning_rate   | 0.0005      |\n","|    n_updates       | 16019       |\n","|    reward          | -0.42198265 |\n","------------------------------------\n","Episode: 19\n","======ddpg Validation from:  2023-01-04 to  2023-04-05\n","Episode: 1\n","ddpg Sharpe Ratio:  0.04109457501412104\n","======ppo Training========\n","{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n","Using cuda device\n","Logging to tensorboard_log//ppo/ppo_378_1\n","Episode: 21\n","-----------------------------------\n","| time/              |            |\n","|    fps             | 100        |\n","|    iterations      | 1          |\n","|    time_elapsed    | 20         |\n","|    total_timesteps | 2048       |\n","| train/             |            |\n","|    reward          | 0.50160056 |\n","-----------------------------------\n","Episode: 22\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 102        |\n","|    iterations           | 2          |\n","|    time_elapsed         | 40         |\n","|    total_timesteps      | 4096       |\n","| train/                  |            |\n","|    approx_kl            | 0.01652199 |\n","|    clip_fraction        | 0.205      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -41.2      |\n","|    explained_variance   | -0.00105   |\n","|    learning_rate        | 0.00025    |\n","|    loss                 | 8.45       |\n","|    n_updates            | 10         |\n","|    policy_gradient_loss | -0.0328    |\n","|    reward               | 0.1963542  |\n","|    std                  | 1          |\n","|    value_loss           | 18         |\n","----------------------------------------\n","Episode: 23\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 102        |\n","|    iterations           | 3          |\n","|    time_elapsed         | 60         |\n","|    total_timesteps      | 6144       |\n","| train/                  |            |\n","|    approx_kl            | 0.01655744 |\n","|    clip_fraction        | 0.203      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -41.2      |\n","|    explained_variance   | -0.0022    |\n","|    learning_rate        | 0.00025    |\n","|    loss                 | 11.2       |\n","|    n_updates            | 20         |\n","|    policy_gradient_loss | -0.0318    |\n","|    reward               | 0.7551121  |\n","|    std                  | 1          |\n","|    value_loss           | 19.4       |\n","----------------------------------------\n","Episode: 24\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 101         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 81          |\n","|    total_timesteps      | 8192        |\n","| train/                  |             |\n","|    approx_kl            | 0.017380076 |\n","|    clip_fraction        | 0.209       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.3       |\n","|    explained_variance   | -0.0223     |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 12.7        |\n","|    n_updates            | 30          |\n","|    policy_gradient_loss | -0.0317     |\n","|    reward               | 0.46516347  |\n","|    std                  | 1.01        |\n","|    value_loss           | 16.1        |\n","-----------------------------------------\n","Episode: 25\n","day: 2014, episode: 25\n","begin_total_asset: 1000000.00\n","end_total_asset: 1684349.67\n","total_reward: 684349.67\n","total_cost: 322895.29\n","total_trades: 53654\n","Sharpe: 0.445\n","=================================\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 99          |\n","|    iterations           | 5           |\n","|    time_elapsed         | 102         |\n","|    total_timesteps      | 10240       |\n","| train/                  |             |\n","|    approx_kl            | 0.015296997 |\n","|    clip_fraction        | 0.17        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.4       |\n","|    explained_variance   | -0.00159    |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 9.22        |\n","|    n_updates            | 40          |\n","|    policy_gradient_loss | -0.0261     |\n","|    reward               | -0.07794235 |\n","|    std                  | 1.01        |\n","|    value_loss           | 16          |\n","-----------------------------------------\n","Episode: 26\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 99          |\n","|    iterations           | 6           |\n","|    time_elapsed         | 124         |\n","|    total_timesteps      | 12288       |\n","| train/                  |             |\n","|    approx_kl            | 0.016512131 |\n","|    clip_fraction        | 0.212       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.5       |\n","|    explained_variance   | -0.00488    |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 6.56        |\n","|    n_updates            | 50          |\n","|    policy_gradient_loss | -0.0305     |\n","|    reward               | 1.8105613   |\n","|    std                  | 1.01        |\n","|    value_loss           | 15.7        |\n","-----------------------------------------\n","Episode: 27\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 99          |\n","|    iterations           | 7           |\n","|    time_elapsed         | 143         |\n","|    total_timesteps      | 14336       |\n","| train/                  |             |\n","|    approx_kl            | 0.017070428 |\n","|    clip_fraction        | 0.192       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.5       |\n","|    explained_variance   | -0.0143     |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 11.4        |\n","|    n_updates            | 60          |\n","|    policy_gradient_loss | -0.0308     |\n","|    reward               | -0.552612   |\n","|    std                  | 1.01        |\n","|    value_loss           | 17          |\n","-----------------------------------------\n","Episode: 28\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 100         |\n","|    iterations           | 8           |\n","|    time_elapsed         | 163         |\n","|    total_timesteps      | 16384       |\n","| train/                  |             |\n","|    approx_kl            | 0.018345252 |\n","|    clip_fraction        | 0.21        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.6       |\n","|    explained_variance   | 0.0232      |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 4.44        |\n","|    n_updates            | 70          |\n","|    policy_gradient_loss | -0.0335     |\n","|    reward               | 0.41081566  |\n","|    std                  | 1.02        |\n","|    value_loss           | 15.1        |\n","-----------------------------------------\n","Episode: 29\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 100         |\n","|    iterations           | 9           |\n","|    time_elapsed         | 183         |\n","|    total_timesteps      | 18432       |\n","| train/                  |             |\n","|    approx_kl            | 0.021811906 |\n","|    clip_fraction        | 0.251       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.6       |\n","|    explained_variance   | 0.0151      |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 4.58        |\n","|    n_updates            | 80          |\n","|    policy_gradient_loss | -0.0331     |\n","|    reward               | 0.38253966  |\n","|    std                  | 1.02        |\n","|    value_loss           | 15.2        |\n","-----------------------------------------\n","Episode: 30\n","day: 2014, episode: 30\n","begin_total_asset: 1000000.00\n","end_total_asset: 1805369.12\n","total_reward: 805369.12\n","total_cost: 329965.10\n","total_trades: 53792\n","Sharpe: 0.506\n","=================================\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 99          |\n","|    iterations           | 10          |\n","|    time_elapsed         | 206         |\n","|    total_timesteps      | 20480       |\n","| train/                  |             |\n","|    approx_kl            | 0.021362789 |\n","|    clip_fraction        | 0.221       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.6       |\n","|    explained_variance   | 0.0253      |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 4.9         |\n","|    n_updates            | 90          |\n","|    policy_gradient_loss | -0.0361     |\n","|    reward               | -0.07165752 |\n","|    std                  | 1.02        |\n","|    value_loss           | 13.4        |\n","-----------------------------------------\n","======ppo Validation from:  2023-01-04 to  2023-04-05\n","Episode: 1\n","ppo Sharpe Ratio:  -0.11752895284511357\n","======Best Model Retraining from:  2015-01-01 to  2023-04-05\n","======Trading from:  2023-04-05 to  2023-07-07\n","Used Model:  <stable_baselines3.ddpg.ddpg.DDPG object at 0x7a99d3102e30>\n","Episode: 1\n","============================================\n","turbulence_threshold:  182.0098142753525\n","======Model training from:  2015-01-01 to  2023-04-05\n","==============Model Training===========\n","======a2c Training========\n","{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n","Using cuda device\n","Logging to tensorboard_log//a2c/a2c_441_1\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 104        |\n","|    iterations         | 100        |\n","|    time_elapsed       | 4          |\n","|    total_timesteps    | 500        |\n","| train/                |            |\n","|    entropy_loss       | -41.1      |\n","|    explained_variance | 0.00805    |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 99         |\n","|    policy_loss        | 10.1       |\n","|    reward             | 0.06815776 |\n","|    std                | 0.999      |\n","|    value_loss         | 0.13       |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 103        |\n","|    iterations         | 200        |\n","|    time_elapsed       | 9          |\n","|    total_timesteps    | 1000       |\n","| train/                |            |\n","|    entropy_loss       | -41.1      |\n","|    explained_variance | 0.145      |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 199        |\n","|    policy_loss        | -56.4      |\n","|    reward             | -2.2310133 |\n","|    std                | 0.998      |\n","|    value_loss         | 2.64       |\n","--------------------------------------\n","----------------------------------------\n","| time/                 |              |\n","|    fps                | 93           |\n","|    iterations         | 300          |\n","|    time_elapsed       | 15           |\n","|    total_timesteps    | 1500         |\n","| train/                |              |\n","|    entropy_loss       | -41.1        |\n","|    explained_variance | 1.19e-07     |\n","|    learning_rate      | 0.0007       |\n","|    n_updates          | 299          |\n","|    policy_loss        | 3.96         |\n","|    reward             | -0.037203338 |\n","|    std                | 0.997        |\n","|    value_loss         | 0.577        |\n","----------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 96       |\n","|    iterations         | 400      |\n","|    time_elapsed       | 20       |\n","|    total_timesteps    | 2000     |\n","| train/                |          |\n","|    entropy_loss       | -41.1    |\n","|    explained_variance | 0        |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 399      |\n","|    policy_loss        | -39.8    |\n","|    reward             | 4.932937 |\n","|    std                | 0.997    |\n","|    value_loss         | 11.5     |\n","------------------------------------\n","Episode: 1\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 93          |\n","|    iterations         | 500         |\n","|    time_elapsed       | 26          |\n","|    total_timesteps    | 2500        |\n","| train/                |             |\n","|    entropy_loss       | -41.1       |\n","|    explained_variance | 0.15        |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 499         |\n","|    policy_loss        | 4.12        |\n","|    reward             | -0.09127487 |\n","|    std                | 0.998       |\n","|    value_loss         | 0.168       |\n","---------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 93         |\n","|    iterations         | 600        |\n","|    time_elapsed       | 32         |\n","|    total_timesteps    | 3000       |\n","| train/                |            |\n","|    entropy_loss       | -41.1      |\n","|    explained_variance | 0.356      |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 599        |\n","|    policy_loss        | 42.4       |\n","|    reward             | -0.9131881 |\n","|    std                | 1          |\n","|    value_loss         | 1.17       |\n","--------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 94       |\n","|    iterations         | 700      |\n","|    time_elapsed       | 36       |\n","|    total_timesteps    | 3500     |\n","| train/                |          |\n","|    entropy_loss       | -41.1    |\n","|    explained_variance | -0.00547 |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 699      |\n","|    policy_loss        | 27.2     |\n","|    reward             | 2.187192 |\n","|    std                | 0.999    |\n","|    value_loss         | 0.812    |\n","------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 91        |\n","|    iterations         | 800       |\n","|    time_elapsed       | 43        |\n","|    total_timesteps    | 4000      |\n","| train/                |           |\n","|    entropy_loss       | -41.1     |\n","|    explained_variance | 0.0656    |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 799       |\n","|    policy_loss        | 206       |\n","|    reward             | -4.670504 |\n","|    std                | 0.997     |\n","|    value_loss         | 29.6      |\n","-------------------------------------\n","Episode: 2\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 93        |\n","|    iterations         | 900       |\n","|    time_elapsed       | 48        |\n","|    total_timesteps    | 4500      |\n","| train/                |           |\n","|    entropy_loss       | -41       |\n","|    explained_variance | -0.0409   |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 899       |\n","|    policy_loss        | -19.1     |\n","|    reward             | 1.2303996 |\n","|    std                | 0.996     |\n","|    value_loss         | 1.02      |\n","-------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 93         |\n","|    iterations         | 1000       |\n","|    time_elapsed       | 53         |\n","|    total_timesteps    | 5000       |\n","| train/                |            |\n","|    entropy_loss       | -41        |\n","|    explained_variance | -0.281     |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 999        |\n","|    policy_loss        | -63.7      |\n","|    reward             | 0.72795606 |\n","|    std                | 0.996      |\n","|    value_loss         | 3.18       |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 92         |\n","|    iterations         | 1100       |\n","|    time_elapsed       | 59         |\n","|    total_timesteps    | 5500       |\n","| train/                |            |\n","|    entropy_loss       | -41        |\n","|    explained_variance | 0          |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 1099       |\n","|    policy_loss        | 464        |\n","|    reward             | -0.6500587 |\n","|    std                | 0.994      |\n","|    value_loss         | 142        |\n","--------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 93        |\n","|    iterations         | 1200      |\n","|    time_elapsed       | 64        |\n","|    total_timesteps    | 6000      |\n","| train/                |           |\n","|    entropy_loss       | -41       |\n","|    explained_variance | 0         |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 1199      |\n","|    policy_loss        | -392      |\n","|    reward             | -8.424189 |\n","|    std                | 0.994     |\n","|    value_loss         | 102       |\n","-------------------------------------\n","Episode: 3\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 92         |\n","|    iterations         | 1300       |\n","|    time_elapsed       | 69         |\n","|    total_timesteps    | 6500       |\n","| train/                |            |\n","|    entropy_loss       | -40.9      |\n","|    explained_variance | 0.00347    |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 1299       |\n","|    policy_loss        | -88.6      |\n","|    reward             | -1.9087869 |\n","|    std                | 0.992      |\n","|    value_loss         | 5.29       |\n","--------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 92        |\n","|    iterations         | 1400      |\n","|    time_elapsed       | 75        |\n","|    total_timesteps    | 7000      |\n","| train/                |           |\n","|    entropy_loss       | -40.9     |\n","|    explained_variance | 0         |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 1399      |\n","|    policy_loss        | 87.1      |\n","|    reward             | 1.3556615 |\n","|    std                | 0.992     |\n","|    value_loss         | 10.1      |\n","-------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 93        |\n","|    iterations         | 1500      |\n","|    time_elapsed       | 80        |\n","|    total_timesteps    | 7500      |\n","| train/                |           |\n","|    entropy_loss       | -41       |\n","|    explained_variance | 0.0736    |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 1499      |\n","|    policy_loss        | -70.2     |\n","|    reward             | 0.7106412 |\n","|    std                | 0.994     |\n","|    value_loss         | 4.77      |\n","-------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 92         |\n","|    iterations         | 1600       |\n","|    time_elapsed       | 86         |\n","|    total_timesteps    | 8000       |\n","| train/                |            |\n","|    entropy_loss       | -40.9      |\n","|    explained_variance | -1.19e-07  |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 1599       |\n","|    policy_loss        | 10.2       |\n","|    reward             | 0.33739445 |\n","|    std                | 0.993      |\n","|    value_loss         | 0.836      |\n","--------------------------------------\n","Episode: 4\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 93        |\n","|    iterations         | 1700      |\n","|    time_elapsed       | 91        |\n","|    total_timesteps    | 8500      |\n","| train/                |           |\n","|    entropy_loss       | -40.9     |\n","|    explained_variance | 0         |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 1699      |\n","|    policy_loss        | -82.9     |\n","|    reward             | 0.5972496 |\n","|    std                | 0.992     |\n","|    value_loss         | 5.04      |\n","-------------------------------------\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 93          |\n","|    iterations         | 1800        |\n","|    time_elapsed       | 96          |\n","|    total_timesteps    | 9000        |\n","| train/                |             |\n","|    entropy_loss       | -40.9       |\n","|    explained_variance | -0.0361     |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 1799        |\n","|    policy_loss        | -25.2       |\n","|    reward             | -0.06117223 |\n","|    std                | 0.993       |\n","|    value_loss         | 0.633       |\n","---------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 91         |\n","|    iterations         | 1900       |\n","|    time_elapsed       | 103        |\n","|    total_timesteps    | 9500       |\n","| train/                |            |\n","|    entropy_loss       | -40.9      |\n","|    explained_variance | -0.0305    |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 1899       |\n","|    policy_loss        | -91.8      |\n","|    reward             | 0.69956934 |\n","|    std                | 0.993      |\n","|    value_loss         | 7.4        |\n","--------------------------------------\n","----------------------------------------\n","| time/                 |              |\n","|    fps                | 91           |\n","|    iterations         | 2000         |\n","|    time_elapsed       | 109          |\n","|    total_timesteps    | 10000        |\n","| train/                |              |\n","|    entropy_loss       | -41          |\n","|    explained_variance | -0.000343    |\n","|    learning_rate      | 0.0007       |\n","|    n_updates          | 1999         |\n","|    policy_loss        | -38.9        |\n","|    reward             | 0.0057028574 |\n","|    std                | 0.995        |\n","|    value_loss         | 3.89         |\n","----------------------------------------\n","Episode: 5\n","day: 2077, episode: 5\n","begin_total_asset: 1000000.00\n","end_total_asset: 2910277.62\n","total_reward: 1910277.62\n","total_cost: 7704.18\n","total_trades: 32203\n","Sharpe: 0.702\n","=================================\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 90         |\n","|    iterations         | 2100       |\n","|    time_elapsed       | 115        |\n","|    total_timesteps    | 10500      |\n","| train/                |            |\n","|    entropy_loss       | -41        |\n","|    explained_variance | -0.0491    |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 2099       |\n","|    policy_loss        | -50.9      |\n","|    reward             | 0.15205733 |\n","|    std                | 0.996      |\n","|    value_loss         | 1.64       |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 91         |\n","|    iterations         | 2200       |\n","|    time_elapsed       | 120        |\n","|    total_timesteps    | 11000      |\n","| train/                |            |\n","|    entropy_loss       | -41        |\n","|    explained_variance | -1.19e-07  |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 2199       |\n","|    policy_loss        | -4.08      |\n","|    reward             | -1.2628757 |\n","|    std                | 0.995      |\n","|    value_loss         | 0.229      |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 91         |\n","|    iterations         | 2300       |\n","|    time_elapsed       | 125        |\n","|    total_timesteps    | 11500      |\n","| train/                |            |\n","|    entropy_loss       | -41.1      |\n","|    explained_variance | -1.19e-07  |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 2299       |\n","|    policy_loss        | -50.8      |\n","|    reward             | -1.9376904 |\n","|    std                | 0.998      |\n","|    value_loss         | 3.08       |\n","--------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 91        |\n","|    iterations         | 2400      |\n","|    time_elapsed       | 131       |\n","|    total_timesteps    | 12000     |\n","| train/                |           |\n","|    entropy_loss       | -41.1     |\n","|    explained_variance | 0         |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 2399      |\n","|    policy_loss        | -22.4     |\n","|    reward             | -2.126181 |\n","|    std                | 0.998     |\n","|    value_loss         | 9.59      |\n","-------------------------------------\n","Episode: 6\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 91          |\n","|    iterations         | 2500        |\n","|    time_elapsed       | 136         |\n","|    total_timesteps    | 12500       |\n","| train/                |             |\n","|    entropy_loss       | -41.1       |\n","|    explained_variance | 0.0813      |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 2499        |\n","|    policy_loss        | 43.2        |\n","|    reward             | -0.29741117 |\n","|    std                | 1           |\n","|    value_loss         | 1.33        |\n","---------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 91         |\n","|    iterations         | 2600       |\n","|    time_elapsed       | 141        |\n","|    total_timesteps    | 13000      |\n","| train/                |            |\n","|    entropy_loss       | -41.2      |\n","|    explained_variance | 0          |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 2599       |\n","|    policy_loss        | 70.2       |\n","|    reward             | 0.79243404 |\n","|    std                | 1          |\n","|    value_loss         | 3.98       |\n","--------------------------------------\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 91          |\n","|    iterations         | 2700        |\n","|    time_elapsed       | 147         |\n","|    total_timesteps    | 13500       |\n","| train/                |             |\n","|    entropy_loss       | -41.2       |\n","|    explained_variance | -0.00927    |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 2699        |\n","|    policy_loss        | 153         |\n","|    reward             | 0.045669068 |\n","|    std                | 1           |\n","|    value_loss         | 20.7        |\n","---------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 92         |\n","|    iterations         | 2800       |\n","|    time_elapsed       | 151        |\n","|    total_timesteps    | 14000      |\n","| train/                |            |\n","|    entropy_loss       | -41.2      |\n","|    explained_variance | -1.19e-07  |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 2799       |\n","|    policy_loss        | -527       |\n","|    reward             | -1.7849677 |\n","|    std                | 1          |\n","|    value_loss         | 156        |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 91         |\n","|    iterations         | 2900       |\n","|    time_elapsed       | 158        |\n","|    total_timesteps    | 14500      |\n","| train/                |            |\n","|    entropy_loss       | -41.2      |\n","|    explained_variance | -8.94e-05  |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 2899       |\n","|    policy_loss        | 109        |\n","|    reward             | -3.6341684 |\n","|    std                | 1          |\n","|    value_loss         | 15.3       |\n","--------------------------------------\n","Episode: 7\n","----------------------------------------\n","| time/                 |              |\n","|    fps                | 91           |\n","|    iterations         | 3000         |\n","|    time_elapsed       | 163          |\n","|    total_timesteps    | 15000        |\n","| train/                |              |\n","|    entropy_loss       | -41.2        |\n","|    explained_variance | -0.408       |\n","|    learning_rate      | 0.0007       |\n","|    n_updates          | 2999         |\n","|    policy_loss        | -75.3        |\n","|    reward             | -0.024939658 |\n","|    std                | 1            |\n","|    value_loss         | 4.61         |\n","----------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 92        |\n","|    iterations         | 3100      |\n","|    time_elapsed       | 167       |\n","|    total_timesteps    | 15500     |\n","| train/                |           |\n","|    entropy_loss       | -41.3     |\n","|    explained_variance | -2.38e-07 |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 3099      |\n","|    policy_loss        | -191      |\n","|    reward             | 6.778903  |\n","|    std                | 1.01      |\n","|    value_loss         | 34.8      |\n","-------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 91        |\n","|    iterations         | 3200      |\n","|    time_elapsed       | 174       |\n","|    total_timesteps    | 16000     |\n","| train/                |           |\n","|    entropy_loss       | -41.3     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 3199      |\n","|    policy_loss        | 199       |\n","|    reward             | 0.5026254 |\n","|    std                | 1.01      |\n","|    value_loss         | 27.7      |\n","-------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 92        |\n","|    iterations         | 3300      |\n","|    time_elapsed       | 179       |\n","|    total_timesteps    | 16500     |\n","| train/                |           |\n","|    entropy_loss       | -41.3     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 3299      |\n","|    policy_loss        | -53.6     |\n","|    reward             | -3.364967 |\n","|    std                | 1.01      |\n","|    value_loss         | 14        |\n","-------------------------------------\n","Episode: 8\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 92          |\n","|    iterations         | 3400        |\n","|    time_elapsed       | 184         |\n","|    total_timesteps    | 17000       |\n","| train/                |             |\n","|    entropy_loss       | -41.3       |\n","|    explained_variance | -0.111      |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 3399        |\n","|    policy_loss        | 7.99        |\n","|    reward             | 0.073581815 |\n","|    std                | 1.01        |\n","|    value_loss         | 0.291       |\n","---------------------------------------\n","----------------------------------------\n","| time/                 |              |\n","|    fps                | 91           |\n","|    iterations         | 3500         |\n","|    time_elapsed       | 190          |\n","|    total_timesteps    | 17500        |\n","| train/                |              |\n","|    entropy_loss       | -41.3        |\n","|    explained_variance | -0.00151     |\n","|    learning_rate      | 0.0007       |\n","|    n_updates          | 3499         |\n","|    policy_loss        | -48.1        |\n","|    reward             | -0.028598083 |\n","|    std                | 1.01         |\n","|    value_loss         | 1.83         |\n","----------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 92        |\n","|    iterations         | 3600      |\n","|    time_elapsed       | 195       |\n","|    total_timesteps    | 18000     |\n","| train/                |           |\n","|    entropy_loss       | -41.3     |\n","|    explained_variance | 0.00313   |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 3599      |\n","|    policy_loss        | -397      |\n","|    reward             | 2.1015632 |\n","|    std                | 1.01      |\n","|    value_loss         | 176       |\n","-------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 91        |\n","|    iterations         | 3700      |\n","|    time_elapsed       | 201       |\n","|    total_timesteps    | 18500     |\n","| train/                |           |\n","|    entropy_loss       | -41.3     |\n","|    explained_variance | 0         |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 3699      |\n","|    policy_loss        | 38.3      |\n","|    reward             | 2.8156796 |\n","|    std                | 1.01      |\n","|    value_loss         | 8.06      |\n","-------------------------------------\n","Episode: 9\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 91         |\n","|    iterations         | 3800       |\n","|    time_elapsed       | 206        |\n","|    total_timesteps    | 19000      |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | 0.0495     |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3799       |\n","|    policy_loss        | 9.88       |\n","|    reward             | 0.31707034 |\n","|    std                | 1.01       |\n","|    value_loss         | 0.717      |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 92         |\n","|    iterations         | 3900       |\n","|    time_elapsed       | 211        |\n","|    total_timesteps    | 19500      |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | 0          |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3899       |\n","|    policy_loss        | 81.7       |\n","|    reward             | -1.3830494 |\n","|    std                | 1.01       |\n","|    value_loss         | 9.85       |\n","--------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 91        |\n","|    iterations         | 4000      |\n","|    time_elapsed       | 219       |\n","|    total_timesteps    | 20000     |\n","| train/                |           |\n","|    entropy_loss       | -41.3     |\n","|    explained_variance | 0.0177    |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 3999      |\n","|    policy_loss        | -797      |\n","|    reward             | 14.636087 |\n","|    std                | 1.01      |\n","|    value_loss         | 345       |\n","-------------------------------------\n","======a2c Validation from:  2023-04-05 to  2023-07-07\n","Episode: 1\n","a2c Sharpe Ratio:  -0.06649924689916284\n","======ddpg Training========\n","{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n","Using cuda device\n","Logging to tensorboard_log//ddpg/ddpg_441_1\n","Episode: 11\n","Episode: 12\n","Episode: 13\n","Episode: 14\n","------------------------------------\n","| time/              |             |\n","|    episodes        | 4           |\n","|    fps             | 57          |\n","|    time_elapsed    | 145         |\n","|    total_timesteps | 8312        |\n","| train/             |             |\n","|    actor_loss      | 1.26        |\n","|    critic_loss     | 55.4        |\n","|    learning_rate   | 0.0005      |\n","|    n_updates       | 8211        |\n","|    reward          | -0.24702004 |\n","------------------------------------\n","Episode: 15\n","day: 2077, episode: 15\n","begin_total_asset: 1000000.00\n","end_total_asset: 2209427.69\n","total_reward: 1209427.69\n","total_cost: 1230.50\n","total_trades: 29116\n","Sharpe: 0.602\n","=================================\n","Episode: 16\n","Episode: 17\n","Episode: 18\n","------------------------------------\n","| time/              |             |\n","|    episodes        | 8           |\n","|    fps             | 56          |\n","|    time_elapsed    | 296         |\n","|    total_timesteps | 16624       |\n","| train/             |             |\n","|    actor_loss      | -2.76       |\n","|    critic_loss     | 1.21        |\n","|    learning_rate   | 0.0005      |\n","|    n_updates       | 16523       |\n","|    reward          | -0.24702004 |\n","------------------------------------\n","Episode: 19\n","======ddpg Validation from:  2023-04-05 to  2023-07-07\n","Episode: 1\n","ddpg Sharpe Ratio:  0.02913844642150749\n","======ppo Training========\n","{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n","Using cuda device\n","Logging to tensorboard_log//ppo/ppo_441_1\n","-----------------------------------\n","| time/              |            |\n","|    fps             | 104        |\n","|    iterations      | 1          |\n","|    time_elapsed    | 19         |\n","|    total_timesteps | 2048       |\n","| train/             |            |\n","|    reward          | -0.5570216 |\n","-----------------------------------\n","Episode: 21\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 97           |\n","|    iterations           | 2            |\n","|    time_elapsed         | 42           |\n","|    total_timesteps      | 4096         |\n","| train/                  |              |\n","|    approx_kl            | 0.0171055    |\n","|    clip_fraction        | 0.205        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -41.2        |\n","|    explained_variance   | -0.007       |\n","|    learning_rate        | 0.00025      |\n","|    loss                 | 9.25         |\n","|    n_updates            | 10           |\n","|    policy_gradient_loss | -0.0314      |\n","|    reward               | -0.084694445 |\n","|    std                  | 1            |\n","|    value_loss           | 16.8         |\n","------------------------------------------\n","Episode: 22\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 98         |\n","|    iterations           | 3          |\n","|    time_elapsed         | 62         |\n","|    total_timesteps      | 6144       |\n","| train/                  |            |\n","|    approx_kl            | 0.01729424 |\n","|    clip_fraction        | 0.215      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -41.2      |\n","|    explained_variance   | 0.0199     |\n","|    learning_rate        | 0.00025    |\n","|    loss                 | 8.04       |\n","|    n_updates            | 20         |\n","|    policy_gradient_loss | -0.0295    |\n","|    reward               | 0.8803875  |\n","|    std                  | 1          |\n","|    value_loss           | 15.6       |\n","----------------------------------------\n","Episode: 23\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 96          |\n","|    iterations           | 4           |\n","|    time_elapsed         | 84          |\n","|    total_timesteps      | 8192        |\n","| train/                  |             |\n","|    approx_kl            | 0.016299952 |\n","|    clip_fraction        | 0.193       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.3       |\n","|    explained_variance   | 0.0343      |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 9.13        |\n","|    n_updates            | 30          |\n","|    policy_gradient_loss | -0.0279     |\n","|    reward               | -0.23448472 |\n","|    std                  | 1.01        |\n","|    value_loss           | 18.9        |\n","-----------------------------------------\n","Episode: 24\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 95          |\n","|    iterations           | 5           |\n","|    time_elapsed         | 107         |\n","|    total_timesteps      | 10240       |\n","| train/                  |             |\n","|    approx_kl            | 0.014839396 |\n","|    clip_fraction        | 0.136       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.3       |\n","|    explained_variance   | -0.0412     |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 7.53        |\n","|    n_updates            | 40          |\n","|    policy_gradient_loss | -0.0254     |\n","|    reward               | -1.8381082  |\n","|    std                  | 1.01        |\n","|    value_loss           | 16.5        |\n","-----------------------------------------\n","Episode: 25\n","day: 2077, episode: 25\n","begin_total_asset: 1000000.00\n","end_total_asset: 1944472.14\n","total_reward: 944472.14\n","total_cost: 346547.36\n","total_trades: 55555\n","Sharpe: 0.543\n","=================================\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 94          |\n","|    iterations           | 6           |\n","|    time_elapsed         | 129         |\n","|    total_timesteps      | 12288       |\n","| train/                  |             |\n","|    approx_kl            | 0.015802056 |\n","|    clip_fraction        | 0.163       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.3       |\n","|    explained_variance   | 0.0295      |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 6.02        |\n","|    n_updates            | 50          |\n","|    policy_gradient_loss | -0.0278     |\n","|    reward               | 4.4390264   |\n","|    std                  | 1.01        |\n","|    value_loss           | 13.1        |\n","-----------------------------------------\n","Episode: 26\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 95          |\n","|    iterations           | 7           |\n","|    time_elapsed         | 149         |\n","|    total_timesteps      | 14336       |\n","| train/                  |             |\n","|    approx_kl            | 0.018040307 |\n","|    clip_fraction        | 0.184       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.4       |\n","|    explained_variance   | 0.00888     |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 9.51        |\n","|    n_updates            | 60          |\n","|    policy_gradient_loss | -0.0273     |\n","|    reward               | -2.412987   |\n","|    std                  | 1.01        |\n","|    value_loss           | 19.1        |\n","-----------------------------------------\n","Episode: 27\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 94          |\n","|    iterations           | 8           |\n","|    time_elapsed         | 172         |\n","|    total_timesteps      | 16384       |\n","| train/                  |             |\n","|    approx_kl            | 0.017389737 |\n","|    clip_fraction        | 0.201       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.5       |\n","|    explained_variance   | 0.0178      |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 9.18        |\n","|    n_updates            | 70          |\n","|    policy_gradient_loss | -0.0236     |\n","|    reward               | -1.2570289  |\n","|    std                  | 1.01        |\n","|    value_loss           | 21.7        |\n","-----------------------------------------\n","Episode: 28\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 95          |\n","|    iterations           | 9           |\n","|    time_elapsed         | 193         |\n","|    total_timesteps      | 18432       |\n","| train/                  |             |\n","|    approx_kl            | 0.018889604 |\n","|    clip_fraction        | 0.194       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.6       |\n","|    explained_variance   | -0.0443     |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 6.14        |\n","|    n_updates            | 80          |\n","|    policy_gradient_loss | -0.0336     |\n","|    reward               | 4.765969    |\n","|    std                  | 1.02        |\n","|    value_loss           | 15.4        |\n","-----------------------------------------\n","Episode: 29\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 94          |\n","|    iterations           | 10          |\n","|    time_elapsed         | 217         |\n","|    total_timesteps      | 20480       |\n","| train/                  |             |\n","|    approx_kl            | 0.017065573 |\n","|    clip_fraction        | 0.193       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.7       |\n","|    explained_variance   | 0.0262      |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 8.62        |\n","|    n_updates            | 90          |\n","|    policy_gradient_loss | -0.0297     |\n","|    reward               | -1.4962801  |\n","|    std                  | 1.02        |\n","|    value_loss           | 18          |\n","-----------------------------------------\n","======ppo Validation from:  2023-04-05 to  2023-07-07\n","Episode: 1\n","ppo Sharpe Ratio:  -0.04620413403541566\n","======Best Model Retraining from:  2015-01-01 to  2023-07-07\n","======Trading from:  2023-07-07 to  2023-10-05\n","Used Model:  <stable_baselines3.ddpg.ddpg.DDPG object at 0x7a99b63a8910>\n","Episode: 1\n","============================================\n","turbulence_threshold:  182.0098142753525\n","======Model training from:  2015-01-01 to  2023-07-07\n","==============Model Training===========\n","======a2c Training========\n","{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n","Using cuda device\n","Logging to tensorboard_log//a2c/a2c_504_1\n","----------------------------------------\n","| time/                 |              |\n","|    fps                | 104          |\n","|    iterations         | 100          |\n","|    time_elapsed       | 4            |\n","|    total_timesteps    | 500          |\n","| train/                |              |\n","|    entropy_loss       | -41.3        |\n","|    explained_variance | 0.145        |\n","|    learning_rate      | 0.0007       |\n","|    n_updates          | 99           |\n","|    policy_loss        | 39.2         |\n","|    reward             | 0.0052659307 |\n","|    std                | 1.01         |\n","|    value_loss         | 0.99         |\n","----------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 89         |\n","|    iterations         | 200        |\n","|    time_elapsed       | 11         |\n","|    total_timesteps    | 1000       |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | -0.0226    |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 199        |\n","|    policy_loss        | -102       |\n","|    reward             | -3.8420875 |\n","|    std                | 1.01       |\n","|    value_loss         | 6.4        |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 89         |\n","|    iterations         | 300        |\n","|    time_elapsed       | 16         |\n","|    total_timesteps    | 1500       |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | 0.0479     |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 299        |\n","|    policy_loss        | -57.6      |\n","|    reward             | -1.0235218 |\n","|    std                | 1.01       |\n","|    value_loss         | 4.12       |\n","--------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 91        |\n","|    iterations         | 400       |\n","|    time_elapsed       | 21        |\n","|    total_timesteps    | 2000      |\n","| train/                |           |\n","|    entropy_loss       | -41.3     |\n","|    explained_variance | 0.0213    |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 399       |\n","|    policy_loss        | -3.36     |\n","|    reward             | 2.6163301 |\n","|    std                | 1.01      |\n","|    value_loss         | 2.24      |\n","-------------------------------------\n","Episode: 1\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 87         |\n","|    iterations         | 500        |\n","|    time_elapsed       | 28         |\n","|    total_timesteps    | 2500       |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | -0.147     |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 499        |\n","|    policy_loss        | 15.4       |\n","|    reward             | 0.01794461 |\n","|    std                | 1.01       |\n","|    value_loss         | 0.66       |\n","--------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 89        |\n","|    iterations         | 600       |\n","|    time_elapsed       | 33        |\n","|    total_timesteps    | 3000      |\n","| train/                |           |\n","|    entropy_loss       | -41.3     |\n","|    explained_variance | -0.00486  |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 599       |\n","|    policy_loss        | 7.4       |\n","|    reward             | 1.0726032 |\n","|    std                | 1.01      |\n","|    value_loss         | 0.234     |\n","-------------------------------------\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 88          |\n","|    iterations         | 700         |\n","|    time_elapsed       | 39          |\n","|    total_timesteps    | 3500        |\n","| train/                |             |\n","|    entropy_loss       | -41.3       |\n","|    explained_variance | -0.0973     |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 699         |\n","|    policy_loss        | 133         |\n","|    reward             | -0.06473651 |\n","|    std                | 1           |\n","|    value_loss         | 17.3        |\n","---------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 87        |\n","|    iterations         | 800       |\n","|    time_elapsed       | 45        |\n","|    total_timesteps    | 4000      |\n","| train/                |           |\n","|    entropy_loss       | -41.3     |\n","|    explained_variance | 1.19e-07  |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 799       |\n","|    policy_loss        | -36.4     |\n","|    reward             | -3.428603 |\n","|    std                | 1.01      |\n","|    value_loss         | 8.94      |\n","-------------------------------------\n","Episode: 2\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 87         |\n","|    iterations         | 900        |\n","|    time_elapsed       | 51         |\n","|    total_timesteps    | 4500       |\n","| train/                |            |\n","|    entropy_loss       | -41.4      |\n","|    explained_variance | -0.0108    |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 899        |\n","|    policy_loss        | 23.3       |\n","|    reward             | -1.7701914 |\n","|    std                | 1.01       |\n","|    value_loss         | 0.484      |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 86         |\n","|    iterations         | 1000       |\n","|    time_elapsed       | 57         |\n","|    total_timesteps    | 5000       |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | -1.19e-07  |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 999        |\n","|    policy_loss        | 6.2        |\n","|    reward             | 0.21277687 |\n","|    std                | 1.01       |\n","|    value_loss         | 0.306      |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 87         |\n","|    iterations         | 1100       |\n","|    time_elapsed       | 62         |\n","|    total_timesteps    | 5500       |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | -0.0254    |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 1099       |\n","|    policy_loss        | 256        |\n","|    reward             | -1.7944682 |\n","|    std                | 1.01       |\n","|    value_loss         | 49.5       |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 87         |\n","|    iterations         | 1200       |\n","|    time_elapsed       | 68         |\n","|    total_timesteps    | 6000       |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | 1.19e-07   |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 1199       |\n","|    policy_loss        | 15.2       |\n","|    reward             | -4.1435485 |\n","|    std                | 1.01       |\n","|    value_loss         | 8.07       |\n","--------------------------------------\n","Episode: 3\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 86         |\n","|    iterations         | 1300       |\n","|    time_elapsed       | 74         |\n","|    total_timesteps    | 6500       |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | -0.137     |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 1299       |\n","|    policy_loss        | -62.4      |\n","|    reward             | -0.2147338 |\n","|    std                | 1.01       |\n","|    value_loss         | 2.43       |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 87         |\n","|    iterations         | 1400       |\n","|    time_elapsed       | 80         |\n","|    total_timesteps    | 7000       |\n","| train/                |            |\n","|    entropy_loss       | -41.4      |\n","|    explained_variance | 0          |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 1399       |\n","|    policy_loss        | -5.71      |\n","|    reward             | 0.10480887 |\n","|    std                | 1.01       |\n","|    value_loss         | 0.0337     |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 86         |\n","|    iterations         | 1500       |\n","|    time_elapsed       | 87         |\n","|    total_timesteps    | 7500       |\n","| train/                |            |\n","|    entropy_loss       | -41.4      |\n","|    explained_variance | -0.15      |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 1499       |\n","|    policy_loss        | -29.8      |\n","|    reward             | 0.64363503 |\n","|    std                | 1.01       |\n","|    value_loss         | 1.6        |\n","--------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 86        |\n","|    iterations         | 1600      |\n","|    time_elapsed       | 92        |\n","|    total_timesteps    | 8000      |\n","| train/                |           |\n","|    entropy_loss       | -41.4     |\n","|    explained_variance | -0.0162   |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 1599      |\n","|    policy_loss        | 200       |\n","|    reward             | 1.3162102 |\n","|    std                | 1.01      |\n","|    value_loss         | 28.1      |\n","-------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 85        |\n","|    iterations         | 1700      |\n","|    time_elapsed       | 99        |\n","|    total_timesteps    | 8500      |\n","| train/                |           |\n","|    entropy_loss       | -41.5     |\n","|    explained_variance | -1.55e-06 |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 1699      |\n","|    policy_loss        | 60.8      |\n","|    reward             | -1.188837 |\n","|    std                | 1.01      |\n","|    value_loss         | 5.53      |\n","-------------------------------------\n","Episode: 4\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 84         |\n","|    iterations         | 1800       |\n","|    time_elapsed       | 106        |\n","|    total_timesteps    | 9000       |\n","| train/                |            |\n","|    entropy_loss       | -41.5      |\n","|    explained_variance | 0          |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 1799       |\n","|    policy_loss        | -3.83      |\n","|    reward             | -1.3076464 |\n","|    std                | 1.01       |\n","|    value_loss         | 0.541      |\n","--------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 84       |\n","|    iterations         | 1900     |\n","|    time_elapsed       | 112      |\n","|    total_timesteps    | 9500     |\n","| train/                |          |\n","|    entropy_loss       | -41.5    |\n","|    explained_variance | 0.000391 |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 1899     |\n","|    policy_loss        | 99.4     |\n","|    reward             | 2.407197 |\n","|    std                | 1.01     |\n","|    value_loss         | 6.29     |\n","------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 84         |\n","|    iterations         | 2000       |\n","|    time_elapsed       | 118        |\n","|    total_timesteps    | 10000      |\n","| train/                |            |\n","|    entropy_loss       | -41.5      |\n","|    explained_variance | 0.0219     |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 1999       |\n","|    policy_loss        | -113       |\n","|    reward             | 0.27062145 |\n","|    std                | 1.01       |\n","|    value_loss         | 26.1       |\n","--------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 85        |\n","|    iterations         | 2100      |\n","|    time_elapsed       | 123       |\n","|    total_timesteps    | 10500     |\n","| train/                |           |\n","|    entropy_loss       | -41.5     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 2099      |\n","|    policy_loss        | -104      |\n","|    reward             | 2.4658077 |\n","|    std                | 1.01      |\n","|    value_loss         | 8.49      |\n","-------------------------------------\n","Episode: 5\n","day: 2140, episode: 5\n","begin_total_asset: 1000000.00\n","end_total_asset: 2089731.90\n","total_reward: 1089731.90\n","total_cost: 99114.14\n","total_trades: 38302\n","Sharpe: 0.504\n","=================================\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 85          |\n","|    iterations         | 2200        |\n","|    time_elapsed       | 128         |\n","|    total_timesteps    | 11000       |\n","| train/                |             |\n","|    entropy_loss       | -41.5       |\n","|    explained_variance | -0.309      |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 2199        |\n","|    policy_loss        | -15.9       |\n","|    reward             | -0.46716794 |\n","|    std                | 1.01        |\n","|    value_loss         | 0.974       |\n","---------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 85         |\n","|    iterations         | 2300       |\n","|    time_elapsed       | 134        |\n","|    total_timesteps    | 11500      |\n","| train/                |            |\n","|    entropy_loss       | -41.5      |\n","|    explained_variance | 0          |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 2299       |\n","|    policy_loss        | -87.5      |\n","|    reward             | -2.4388912 |\n","|    std                | 1.01       |\n","|    value_loss         | 6.82       |\n","--------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 85        |\n","|    iterations         | 2400      |\n","|    time_elapsed       | 139       |\n","|    total_timesteps    | 12000     |\n","| train/                |           |\n","|    entropy_loss       | -41.5     |\n","|    explained_variance | 0         |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 2399      |\n","|    policy_loss        | -176      |\n","|    reward             | -2.016058 |\n","|    std                | 1.01      |\n","|    value_loss         | 24.2      |\n","-------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 85        |\n","|    iterations         | 2500      |\n","|    time_elapsed       | 145       |\n","|    total_timesteps    | 12500     |\n","| train/                |           |\n","|    entropy_loss       | -41.5     |\n","|    explained_variance | 0         |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 2499      |\n","|    policy_loss        | -110      |\n","|    reward             | 4.3278327 |\n","|    std                | 1.01      |\n","|    value_loss         | 9.46      |\n","-------------------------------------\n","Episode: 6\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 86         |\n","|    iterations         | 2600       |\n","|    time_elapsed       | 150        |\n","|    total_timesteps    | 13000      |\n","| train/                |            |\n","|    entropy_loss       | -41.5      |\n","|    explained_variance | -0.219     |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 2599       |\n","|    policy_loss        | -50.9      |\n","|    reward             | 0.29862967 |\n","|    std                | 1.01       |\n","|    value_loss         | 2.03       |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 86         |\n","|    iterations         | 2700       |\n","|    time_elapsed       | 155        |\n","|    total_timesteps    | 13500      |\n","| train/                |            |\n","|    entropy_loss       | -41.5      |\n","|    explained_variance | -0.00432   |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 2699       |\n","|    policy_loss        | 12.6       |\n","|    reward             | -0.2974505 |\n","|    std                | 1.01       |\n","|    value_loss         | 0.297      |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 86         |\n","|    iterations         | 2800       |\n","|    time_elapsed       | 161        |\n","|    total_timesteps    | 14000      |\n","| train/                |            |\n","|    entropy_loss       | -41.5      |\n","|    explained_variance | 0          |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 2799       |\n","|    policy_loss        | 7.96       |\n","|    reward             | -6.5367618 |\n","|    std                | 1.01       |\n","|    value_loss         | 0.315      |\n","--------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 87        |\n","|    iterations         | 2900      |\n","|    time_elapsed       | 166       |\n","|    total_timesteps    | 14500     |\n","| train/                |           |\n","|    entropy_loss       | -41.5     |\n","|    explained_variance | 0         |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 2899      |\n","|    policy_loss        | 45.5      |\n","|    reward             | 0.7293538 |\n","|    std                | 1.01      |\n","|    value_loss         | 3.89      |\n","-------------------------------------\n","Episode: 7\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 86        |\n","|    iterations         | 3000      |\n","|    time_elapsed       | 172       |\n","|    total_timesteps    | 15000     |\n","| train/                |           |\n","|    entropy_loss       | -41.5     |\n","|    explained_variance | 0         |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 2999      |\n","|    policy_loss        | -30.3     |\n","|    reward             | 1.1126121 |\n","|    std                | 1.01      |\n","|    value_loss         | 0.606     |\n","-------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 87         |\n","|    iterations         | 3100       |\n","|    time_elapsed       | 178        |\n","|    total_timesteps    | 15500      |\n","| train/                |            |\n","|    entropy_loss       | -41.5      |\n","|    explained_variance | 5.96e-08   |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3099       |\n","|    policy_loss        | -40.6      |\n","|    reward             | -0.3329521 |\n","|    std                | 1.01       |\n","|    value_loss         | 1.08       |\n","--------------------------------------\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 87          |\n","|    iterations         | 3200        |\n","|    time_elapsed       | 182         |\n","|    total_timesteps    | 16000       |\n","| train/                |             |\n","|    entropy_loss       | -41.5       |\n","|    explained_variance | -0.0257     |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 3199        |\n","|    policy_loss        | 114         |\n","|    reward             | -0.29417098 |\n","|    std                | 1.01        |\n","|    value_loss         | 10.6        |\n","---------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 87        |\n","|    iterations         | 3300      |\n","|    time_elapsed       | 189       |\n","|    total_timesteps    | 16500     |\n","| train/                |           |\n","|    entropy_loss       | -41.5     |\n","|    explained_variance | 0.118     |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 3299      |\n","|    policy_loss        | -15.6     |\n","|    reward             | 3.1896396 |\n","|    std                | 1.01      |\n","|    value_loss         | 0.564     |\n","-------------------------------------\n","----------------------------------------\n","| time/                 |              |\n","|    fps                | 87           |\n","|    iterations         | 3400         |\n","|    time_elapsed       | 194          |\n","|    total_timesteps    | 17000        |\n","| train/                |              |\n","|    entropy_loss       | -41.5        |\n","|    explained_variance | 0.0518       |\n","|    learning_rate      | 0.0007       |\n","|    n_updates          | 3399         |\n","|    policy_loss        | 44.5         |\n","|    reward             | -0.044235796 |\n","|    std                | 1.01         |\n","|    value_loss         | 4.3          |\n","----------------------------------------\n","Episode: 8\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 87         |\n","|    iterations         | 3500       |\n","|    time_elapsed       | 199        |\n","|    total_timesteps    | 17500      |\n","| train/                |            |\n","|    entropy_loss       | -41.6      |\n","|    explained_variance | 5.96e-08   |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3499       |\n","|    policy_loss        | -19.7      |\n","|    reward             | -4.8655252 |\n","|    std                | 1.02       |\n","|    value_loss         | 0.593      |\n","--------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 87        |\n","|    iterations         | 3600      |\n","|    time_elapsed       | 205       |\n","|    total_timesteps    | 18000     |\n","| train/                |           |\n","|    entropy_loss       | -41.6     |\n","|    explained_variance | 0         |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 3599      |\n","|    policy_loss        | -26.7     |\n","|    reward             | 0.9444712 |\n","|    std                | 1.02      |\n","|    value_loss         | 0.953     |\n","-------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 87        |\n","|    iterations         | 3700      |\n","|    time_elapsed       | 210       |\n","|    total_timesteps    | 18500     |\n","| train/                |           |\n","|    entropy_loss       | -41.6     |\n","|    explained_variance | -0.0033   |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 3699      |\n","|    policy_loss        | 102       |\n","|    reward             | 2.8219364 |\n","|    std                | 1.02      |\n","|    value_loss         | 17.4      |\n","-------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 86         |\n","|    iterations         | 3800       |\n","|    time_elapsed       | 218        |\n","|    total_timesteps    | 19000      |\n","| train/                |            |\n","|    entropy_loss       | -41.6      |\n","|    explained_variance | 0          |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3799       |\n","|    policy_loss        | 140        |\n","|    reward             | -5.0164385 |\n","|    std                | 1.02       |\n","|    value_loss         | 17.4       |\n","--------------------------------------\n","Episode: 9\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 87         |\n","|    iterations         | 3900       |\n","|    time_elapsed       | 223        |\n","|    total_timesteps    | 19500      |\n","| train/                |            |\n","|    entropy_loss       | -41.6      |\n","|    explained_variance | 0.0744     |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3899       |\n","|    policy_loss        | 11.5       |\n","|    reward             | -0.6955845 |\n","|    std                | 1.02       |\n","|    value_loss         | 0.516      |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 87         |\n","|    iterations         | 4000       |\n","|    time_elapsed       | 228        |\n","|    total_timesteps    | 20000      |\n","| train/                |            |\n","|    entropy_loss       | -41.6      |\n","|    explained_variance | 0          |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3999       |\n","|    policy_loss        | 85.9       |\n","|    reward             | -0.5794806 |\n","|    std                | 1.02       |\n","|    value_loss         | 6.54       |\n","--------------------------------------\n","======a2c Validation from:  2023-07-07 to  2023-10-05\n","Episode: 1\n","a2c Sharpe Ratio:  -0.34852958410513796\n","======ddpg Training========\n","{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n","Using cuda device\n","Logging to tensorboard_log//ddpg/ddpg_504_1\n","Episode: 11\n","Episode: 12\n","Episode: 13\n","Episode: 14\n","-----------------------------------\n","| time/              |            |\n","|    episodes        | 4          |\n","|    fps             | 57         |\n","|    time_elapsed    | 148        |\n","|    total_timesteps | 8564       |\n","| train/             |            |\n","|    actor_loss      | 10.9       |\n","|    critic_loss     | 3.37       |\n","|    learning_rate   | 0.0005     |\n","|    n_updates       | 8463       |\n","|    reward          | -1.9670534 |\n","-----------------------------------\n","Episode: 15\n","day: 2140, episode: 15\n","begin_total_asset: 1000000.00\n","end_total_asset: 2338822.44\n","total_reward: 1338822.44\n","total_cost: 998.99\n","total_trades: 34240\n","Sharpe: 0.616\n","=================================\n","Episode: 16\n","Episode: 17\n","Episode: 18\n","-----------------------------------\n","| time/              |            |\n","|    episodes        | 8          |\n","|    fps             | 57         |\n","|    time_elapsed    | 297        |\n","|    total_timesteps | 17128      |\n","| train/             |            |\n","|    actor_loss      | 2.43       |\n","|    critic_loss     | 1.75       |\n","|    learning_rate   | 0.0005     |\n","|    n_updates       | 17027      |\n","|    reward          | -1.9670534 |\n","-----------------------------------\n","Episode: 19\n","======ddpg Validation from:  2023-07-07 to  2023-10-05\n","Episode: 1\n","ddpg Sharpe Ratio:  0.013957855159878857\n","======ppo Training========\n","{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n","Using cuda device\n","Logging to tensorboard_log//ppo/ppo_504_1\n","------------------------------------\n","| time/              |             |\n","|    fps             | 107         |\n","|    iterations      | 1           |\n","|    time_elapsed    | 19          |\n","|    total_timesteps | 2048        |\n","| train/             |             |\n","|    reward          | -0.17214523 |\n","------------------------------------\n","Episode: 21\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 103         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 39          |\n","|    total_timesteps      | 4096        |\n","| train/                  |             |\n","|    approx_kl            | 0.016852958 |\n","|    clip_fraction        | 0.205       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.2       |\n","|    explained_variance   | 0.000304    |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 7.25        |\n","|    n_updates            | 10          |\n","|    policy_gradient_loss | -0.0363     |\n","|    reward               | -3.4975965  |\n","|    std                  | 1           |\n","|    value_loss           | 18          |\n","-----------------------------------------\n","Episode: 22\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 101         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 60          |\n","|    total_timesteps      | 6144        |\n","| train/                  |             |\n","|    approx_kl            | 0.018171666 |\n","|    clip_fraction        | 0.204       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.2       |\n","|    explained_variance   | -0.00555    |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 8.22        |\n","|    n_updates            | 20          |\n","|    policy_gradient_loss | -0.0331     |\n","|    reward               | 1.1619135   |\n","|    std                  | 1           |\n","|    value_loss           | 18.1        |\n","-----------------------------------------\n","Episode: 23\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 102         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 80          |\n","|    total_timesteps      | 8192        |\n","| train/                  |             |\n","|    approx_kl            | 0.016854236 |\n","|    clip_fraction        | 0.207       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.3       |\n","|    explained_variance   | 0.0116      |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 5.91        |\n","|    n_updates            | 30          |\n","|    policy_gradient_loss | -0.0326     |\n","|    reward               | 2.1828845   |\n","|    std                  | 1.01        |\n","|    value_loss           | 15.4        |\n","-----------------------------------------\n","Episode: 24\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 98         |\n","|    iterations           | 5          |\n","|    time_elapsed         | 103        |\n","|    total_timesteps      | 10240      |\n","| train/                  |            |\n","|    approx_kl            | 0.01549993 |\n","|    clip_fraction        | 0.177      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -41.4      |\n","|    explained_variance   | -0.00312   |\n","|    learning_rate        | 0.00025    |\n","|    loss                 | 5.42       |\n","|    n_updates            | 40         |\n","|    policy_gradient_loss | -0.0287    |\n","|    reward               | 0.61508566 |\n","|    std                  | 1.01       |\n","|    value_loss           | 17.1       |\n","----------------------------------------\n","Episode: 25\n","day: 2140, episode: 25\n","begin_total_asset: 1000000.00\n","end_total_asset: 2716617.80\n","total_reward: 1716617.80\n","total_cost: 365544.71\n","total_trades: 57632\n","Sharpe: 0.729\n","=================================\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 99          |\n","|    iterations           | 6           |\n","|    time_elapsed         | 123         |\n","|    total_timesteps      | 12288       |\n","| train/                  |             |\n","|    approx_kl            | 0.014933268 |\n","|    clip_fraction        | 0.154       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.4       |\n","|    explained_variance   | -0.000201   |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 5.43        |\n","|    n_updates            | 50          |\n","|    policy_gradient_loss | -0.0272     |\n","|    reward               | -0.7850793  |\n","|    std                  | 1.01        |\n","|    value_loss           | 19.8        |\n","-----------------------------------------\n","Episode: 26\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 98          |\n","|    iterations           | 7           |\n","|    time_elapsed         | 145         |\n","|    total_timesteps      | 14336       |\n","| train/                  |             |\n","|    approx_kl            | 0.014744099 |\n","|    clip_fraction        | 0.141       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.5       |\n","|    explained_variance   | 0.0295      |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 19.3        |\n","|    n_updates            | 60          |\n","|    policy_gradient_loss | -0.0291     |\n","|    reward               | -1.4084122  |\n","|    std                  | 1.01        |\n","|    value_loss           | 24.6        |\n","-----------------------------------------\n","Episode: 27\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 99          |\n","|    iterations           | 8           |\n","|    time_elapsed         | 164         |\n","|    total_timesteps      | 16384       |\n","| train/                  |             |\n","|    approx_kl            | 0.014004017 |\n","|    clip_fraction        | 0.179       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.6       |\n","|    explained_variance   | 0.0221      |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 7.55        |\n","|    n_updates            | 70          |\n","|    policy_gradient_loss | -0.0273     |\n","|    reward               | 1.1331035   |\n","|    std                  | 1.02        |\n","|    value_loss           | 19.2        |\n","-----------------------------------------\n","Episode: 28\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 99         |\n","|    iterations           | 9          |\n","|    time_elapsed         | 185        |\n","|    total_timesteps      | 18432      |\n","| train/                  |            |\n","|    approx_kl            | 0.01688838 |\n","|    clip_fraction        | 0.198      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -41.6      |\n","|    explained_variance   | 0.0138     |\n","|    learning_rate        | 0.00025    |\n","|    loss                 | 10.7       |\n","|    n_updates            | 80         |\n","|    policy_gradient_loss | -0.0297    |\n","|    reward               | 6.822462   |\n","|    std                  | 1.02       |\n","|    value_loss           | 16.8       |\n","----------------------------------------\n","Episode: 29\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 99          |\n","|    iterations           | 10          |\n","|    time_elapsed         | 206         |\n","|    total_timesteps      | 20480       |\n","| train/                  |             |\n","|    approx_kl            | 0.020585459 |\n","|    clip_fraction        | 0.206       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.7       |\n","|    explained_variance   | 0.0531      |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 6.39        |\n","|    n_updates            | 90          |\n","|    policy_gradient_loss | -0.0302     |\n","|    reward               | 0.15205467  |\n","|    std                  | 1.02        |\n","|    value_loss           | 13.9        |\n","-----------------------------------------\n","======ppo Validation from:  2023-07-07 to  2023-10-05\n","Episode: 1\n","ppo Sharpe Ratio:  -0.23031753399056587\n","======Best Model Retraining from:  2015-01-01 to  2023-10-05\n","======Trading from:  2023-10-05 to  2024-01-05\n","Used Model:  <stable_baselines3.ddpg.ddpg.DDPG object at 0x7a9a12ed6bc0>\n","Episode: 1\n","============================================\n","turbulence_threshold:  182.0098142753525\n","======Model training from:  2015-01-01 to  2023-10-05\n","==============Model Training===========\n","======a2c Training========\n","{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n","Using cuda device\n","Logging to tensorboard_log//a2c/a2c_567_1\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 65          |\n","|    iterations         | 100         |\n","|    time_elapsed       | 7           |\n","|    total_timesteps    | 500         |\n","| train/                |             |\n","|    entropy_loss       | -41.4       |\n","|    explained_variance | -0.406      |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 99          |\n","|    policy_loss        | 15.9        |\n","|    reward             | 0.120186515 |\n","|    std                | 1.01        |\n","|    value_loss         | 0.31        |\n","---------------------------------------\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 79          |\n","|    iterations         | 200         |\n","|    time_elapsed       | 12          |\n","|    total_timesteps    | 1000        |\n","| train/                |             |\n","|    entropy_loss       | -41.4       |\n","|    explained_variance | -0.453      |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 199         |\n","|    policy_loss        | -92.5       |\n","|    reward             | -0.45678467 |\n","|    std                | 1.01        |\n","|    value_loss         | 6.03        |\n","---------------------------------------\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 86          |\n","|    iterations         | 300         |\n","|    time_elapsed       | 17          |\n","|    total_timesteps    | 1500        |\n","| train/                |             |\n","|    entropy_loss       | -41.4       |\n","|    explained_variance | -0.0199     |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 299         |\n","|    policy_loss        | -13.8       |\n","|    reward             | -0.51678586 |\n","|    std                | 1.01        |\n","|    value_loss         | 1.66        |\n","---------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 83        |\n","|    iterations         | 400       |\n","|    time_elapsed       | 23        |\n","|    total_timesteps    | 2000      |\n","| train/                |           |\n","|    entropy_loss       | -41.4     |\n","|    explained_variance | 0         |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 399       |\n","|    policy_loss        | -7.77     |\n","|    reward             | 2.8623025 |\n","|    std                | 1.01      |\n","|    value_loss         | 4.05      |\n","-------------------------------------\n","Episode: 1\n","----------------------------------------\n","| time/                 |              |\n","|    fps                | 86           |\n","|    iterations         | 500          |\n","|    time_elapsed       | 28           |\n","|    total_timesteps    | 2500         |\n","| train/                |              |\n","|    entropy_loss       | -41.4        |\n","|    explained_variance | 0.457        |\n","|    learning_rate      | 0.0007       |\n","|    n_updates          | 499          |\n","|    policy_loss        | 68.7         |\n","|    reward             | -0.042970326 |\n","|    std                | 1.01         |\n","|    value_loss         | 2.8          |\n","----------------------------------------\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 85          |\n","|    iterations         | 600         |\n","|    time_elapsed       | 35          |\n","|    total_timesteps    | 3000        |\n","| train/                |             |\n","|    entropy_loss       | -41.4       |\n","|    explained_variance | -0.0719     |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 599         |\n","|    policy_loss        | -3.24       |\n","|    reward             | 0.005963527 |\n","|    std                | 1.01        |\n","|    value_loss         | 1.15        |\n","---------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 86         |\n","|    iterations         | 700        |\n","|    time_elapsed       | 40         |\n","|    total_timesteps    | 3500       |\n","| train/                |            |\n","|    entropy_loss       | -41.4      |\n","|    explained_variance | 0.168      |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 699        |\n","|    policy_loss        | -62.7      |\n","|    reward             | -5.1952143 |\n","|    std                | 1.01       |\n","|    value_loss         | 2.72       |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 88         |\n","|    iterations         | 800        |\n","|    time_elapsed       | 45         |\n","|    total_timesteps    | 4000       |\n","| train/                |            |\n","|    entropy_loss       | -41.4      |\n","|    explained_variance | 0          |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 799        |\n","|    policy_loss        | -50.7      |\n","|    reward             | -1.5103332 |\n","|    std                | 1.01       |\n","|    value_loss         | 3.29       |\n","--------------------------------------\n","Episode: 2\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 87         |\n","|    iterations         | 900        |\n","|    time_elapsed       | 51         |\n","|    total_timesteps    | 4500       |\n","| train/                |            |\n","|    entropy_loss       | -41.4      |\n","|    explained_variance | -0.0399    |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 899        |\n","|    policy_loss        | 32.4       |\n","|    reward             | 0.18868439 |\n","|    std                | 1.01       |\n","|    value_loss         | 0.763      |\n","--------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 88        |\n","|    iterations         | 1000      |\n","|    time_elapsed       | 56        |\n","|    total_timesteps    | 5000      |\n","| train/                |           |\n","|    entropy_loss       | -41.5     |\n","|    explained_variance | -0.0912   |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 999       |\n","|    policy_loss        | -14.9     |\n","|    reward             | 0.5731054 |\n","|    std                | 1.01      |\n","|    value_loss         | 0.306     |\n","-------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 88         |\n","|    iterations         | 1100       |\n","|    time_elapsed       | 62         |\n","|    total_timesteps    | 5500       |\n","| train/                |            |\n","|    entropy_loss       | -41.5      |\n","|    explained_variance | -0.22      |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 1099       |\n","|    policy_loss        | 50.1       |\n","|    reward             | -1.5508274 |\n","|    std                | 1.01       |\n","|    value_loss         | 1.79       |\n","--------------------------------------\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 88          |\n","|    iterations         | 1200        |\n","|    time_elapsed       | 67          |\n","|    total_timesteps    | 6000        |\n","| train/                |             |\n","|    entropy_loss       | -41.5       |\n","|    explained_variance | 1.79e-07    |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 1199        |\n","|    policy_loss        | 5.41        |\n","|    reward             | -0.29557467 |\n","|    std                | 1.01        |\n","|    value_loss         | 0.643       |\n","---------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 89         |\n","|    iterations         | 1300       |\n","|    time_elapsed       | 72         |\n","|    total_timesteps    | 6500       |\n","| train/                |            |\n","|    entropy_loss       | -41.4      |\n","|    explained_variance | 1.19e-07   |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 1299       |\n","|    policy_loss        | 190        |\n","|    reward             | -0.8503954 |\n","|    std                | 1.01       |\n","|    value_loss         | 19.9       |\n","--------------------------------------\n","Episode: 3\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 88          |\n","|    iterations         | 1400        |\n","|    time_elapsed       | 79          |\n","|    total_timesteps    | 7000        |\n","| train/                |             |\n","|    entropy_loss       | -41.4       |\n","|    explained_variance | -1.19e-07   |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 1399        |\n","|    policy_loss        | 4.95        |\n","|    reward             | -0.22033428 |\n","|    std                | 1.01        |\n","|    value_loss         | 0.128       |\n","---------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 89        |\n","|    iterations         | 1500      |\n","|    time_elapsed       | 84        |\n","|    total_timesteps    | 7500      |\n","| train/                |           |\n","|    entropy_loss       | -41.4     |\n","|    explained_variance | -0.0348   |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 1499      |\n","|    policy_loss        | -0.912    |\n","|    reward             | 1.1859672 |\n","|    std                | 1.01      |\n","|    value_loss         | 0.285     |\n","-------------------------------------\n","----------------------------------------\n","| time/                 |              |\n","|    fps                | 89           |\n","|    iterations         | 1600         |\n","|    time_elapsed       | 89           |\n","|    total_timesteps    | 8000         |\n","| train/                |              |\n","|    entropy_loss       | -41.4        |\n","|    explained_variance | 5.96e-08     |\n","|    learning_rate      | 0.0007       |\n","|    n_updates          | 1599         |\n","|    policy_loss        | -16.5        |\n","|    reward             | -0.054923415 |\n","|    std                | 1.01         |\n","|    value_loss         | 1.73         |\n","----------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 88        |\n","|    iterations         | 1700      |\n","|    time_elapsed       | 95        |\n","|    total_timesteps    | 8500      |\n","| train/                |           |\n","|    entropy_loss       | -41.4     |\n","|    explained_variance | -0.0425   |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 1699      |\n","|    policy_loss        | 223       |\n","|    reward             | -2.689746 |\n","|    std                | 1.01      |\n","|    value_loss         | 33.4      |\n","-------------------------------------\n","Episode: 4\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 89         |\n","|    iterations         | 1800       |\n","|    time_elapsed       | 100        |\n","|    total_timesteps    | 9000       |\n","| train/                |            |\n","|    entropy_loss       | -41.4      |\n","|    explained_variance | -1.19e-07  |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 1799       |\n","|    policy_loss        | -39.2      |\n","|    reward             | 0.87506616 |\n","|    std                | 1.01       |\n","|    value_loss         | 1.48       |\n","--------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 88        |\n","|    iterations         | 1900      |\n","|    time_elapsed       | 106       |\n","|    total_timesteps    | 9500      |\n","| train/                |           |\n","|    entropy_loss       | -41.4     |\n","|    explained_variance | -0.0699   |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 1899      |\n","|    policy_loss        | 77        |\n","|    reward             | 1.0097533 |\n","|    std                | 1.01      |\n","|    value_loss         | 7.34      |\n","-------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 89        |\n","|    iterations         | 2000      |\n","|    time_elapsed       | 111       |\n","|    total_timesteps    | 10000     |\n","| train/                |           |\n","|    entropy_loss       | -41.3     |\n","|    explained_variance | 0         |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 1999      |\n","|    policy_loss        | -137      |\n","|    reward             | 0.9316564 |\n","|    std                | 1.01      |\n","|    value_loss         | 13        |\n","-------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 88         |\n","|    iterations         | 2100       |\n","|    time_elapsed       | 118        |\n","|    total_timesteps    | 10500      |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | -1.19e-07  |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 2099       |\n","|    policy_loss        | 35         |\n","|    reward             | -1.2651066 |\n","|    std                | 1.01       |\n","|    value_loss         | 0.903      |\n","--------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 87       |\n","|    iterations         | 2200     |\n","|    time_elapsed       | 125      |\n","|    total_timesteps    | 11000    |\n","| train/                |          |\n","|    entropy_loss       | -41.3    |\n","|    explained_variance | 0.112    |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 2199     |\n","|    policy_loss        | 3.8      |\n","|    reward             | 0.720747 |\n","|    std                | 1.01     |\n","|    value_loss         | 0.113    |\n","------------------------------------\n","Episode: 5\n","day: 2203, episode: 5\n","begin_total_asset: 1000000.00\n","end_total_asset: 1958323.88\n","total_reward: 958323.88\n","total_cost: 38769.76\n","total_trades: 36630\n","Sharpe: 0.505\n","=================================\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 88         |\n","|    iterations         | 2300       |\n","|    time_elapsed       | 130        |\n","|    total_timesteps    | 11500      |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | 0          |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 2299       |\n","|    policy_loss        | 1.96       |\n","|    reward             | 0.27451992 |\n","|    std                | 1.01       |\n","|    value_loss         | 0.269      |\n","--------------------------------------\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 88          |\n","|    iterations         | 2400        |\n","|    time_elapsed       | 136         |\n","|    total_timesteps    | 12000       |\n","| train/                |             |\n","|    entropy_loss       | -41.3       |\n","|    explained_variance | 0.0157      |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 2399        |\n","|    policy_loss        | -230        |\n","|    reward             | -0.68257046 |\n","|    std                | 1.01        |\n","|    value_loss         | 35.4        |\n","---------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 88        |\n","|    iterations         | 2500      |\n","|    time_elapsed       | 141       |\n","|    total_timesteps    | 12500     |\n","| train/                |           |\n","|    entropy_loss       | -41.3     |\n","|    explained_variance | -0.0108   |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 2499      |\n","|    policy_loss        | 372       |\n","|    reward             | -1.213663 |\n","|    std                | 1         |\n","|    value_loss         | 91.9      |\n","-------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 88         |\n","|    iterations         | 2600       |\n","|    time_elapsed       | 146        |\n","|    total_timesteps    | 13000      |\n","| train/                |            |\n","|    entropy_loss       | -41.2      |\n","|    explained_variance | 0.0457     |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 2599       |\n","|    policy_loss        | 0.851      |\n","|    reward             | -1.2584541 |\n","|    std                | 1          |\n","|    value_loss         | 1.11       |\n","--------------------------------------\n","Episode: 6\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 87         |\n","|    iterations         | 2700       |\n","|    time_elapsed       | 153        |\n","|    total_timesteps    | 13500      |\n","| train/                |            |\n","|    entropy_loss       | -41.2      |\n","|    explained_variance | 0.235      |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 2699       |\n","|    policy_loss        | 97.1       |\n","|    reward             | -0.3524588 |\n","|    std                | 1          |\n","|    value_loss         | 7.55       |\n","--------------------------------------\n","----------------------------------------\n","| time/                 |              |\n","|    fps                | 88           |\n","|    iterations         | 2800         |\n","|    time_elapsed       | 158          |\n","|    total_timesteps    | 14000        |\n","| train/                |              |\n","|    entropy_loss       | -41.3        |\n","|    explained_variance | 0.0568       |\n","|    learning_rate      | 0.0007       |\n","|    n_updates          | 2799         |\n","|    policy_loss        | 211          |\n","|    reward             | -0.036082488 |\n","|    std                | 1            |\n","|    value_loss         | 37.5         |\n","----------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 88         |\n","|    iterations         | 2900       |\n","|    time_elapsed       | 163        |\n","|    total_timesteps    | 14500      |\n","| train/                |            |\n","|    entropy_loss       | -41.2      |\n","|    explained_variance | 1.19e-07   |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 2899       |\n","|    policy_loss        | 106        |\n","|    reward             | -0.9449441 |\n","|    std                | 1          |\n","|    value_loss         | 7.85       |\n","--------------------------------------\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 88        |\n","|    iterations         | 3000      |\n","|    time_elapsed       | 169       |\n","|    total_timesteps    | 15000     |\n","| train/                |           |\n","|    entropy_loss       | -41.2     |\n","|    explained_variance | -1.19e-07 |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 2999      |\n","|    policy_loss        | -22       |\n","|    reward             | -2.525695 |\n","|    std                | 1         |\n","|    value_loss         | 1.81      |\n","-------------------------------------\n","Episode: 7\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 88         |\n","|    iterations         | 3100       |\n","|    time_elapsed       | 174        |\n","|    total_timesteps    | 15500      |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | -0.501     |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3099       |\n","|    policy_loss        | 25.7       |\n","|    reward             | -2.2200282 |\n","|    std                | 1          |\n","|    value_loss         | 0.516      |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 88         |\n","|    iterations         | 3200       |\n","|    time_elapsed       | 181        |\n","|    total_timesteps    | 16000      |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | -1.19e-07  |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3199       |\n","|    policy_loss        | -19.7      |\n","|    reward             | 0.08440637 |\n","|    std                | 1          |\n","|    value_loss         | 0.536      |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 88         |\n","|    iterations         | 3300       |\n","|    time_elapsed       | 186        |\n","|    total_timesteps    | 16500      |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | 0          |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3299       |\n","|    policy_loss        | 78.5       |\n","|    reward             | -0.5578678 |\n","|    std                | 1          |\n","|    value_loss         | 4.34       |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 89         |\n","|    iterations         | 3400       |\n","|    time_elapsed       | 190        |\n","|    total_timesteps    | 17000      |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | -0.00972   |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3399       |\n","|    policy_loss        | 123        |\n","|    reward             | 0.22642942 |\n","|    std                | 1.01       |\n","|    value_loss         | 24.7       |\n","--------------------------------------\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 88          |\n","|    iterations         | 3500        |\n","|    time_elapsed       | 197         |\n","|    total_timesteps    | 17500       |\n","| train/                |             |\n","|    entropy_loss       | -41.3       |\n","|    explained_variance | 0           |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 3499        |\n","|    policy_loss        | 127         |\n","|    reward             | -0.49007824 |\n","|    std                | 1.01        |\n","|    value_loss         | 18.9        |\n","---------------------------------------\n","Episode: 8\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 88        |\n","|    iterations         | 3600      |\n","|    time_elapsed       | 202       |\n","|    total_timesteps    | 18000     |\n","| train/                |           |\n","|    entropy_loss       | -41.3     |\n","|    explained_variance | 0.0029    |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 3599      |\n","|    policy_loss        | -43.3     |\n","|    reward             | 0.9947004 |\n","|    std                | 1.01      |\n","|    value_loss         | 1.38      |\n","-------------------------------------\n","---------------------------------------\n","| time/                 |             |\n","|    fps                | 88          |\n","|    iterations         | 3700        |\n","|    time_elapsed       | 208         |\n","|    total_timesteps    | 18500       |\n","| train/                |             |\n","|    entropy_loss       | -41.3       |\n","|    explained_variance | 1.19e-07    |\n","|    learning_rate      | 0.0007      |\n","|    n_updates          | 3699        |\n","|    policy_loss        | 91.8        |\n","|    reward             | -0.51513684 |\n","|    std                | 1.01        |\n","|    value_loss         | 5.58        |\n","---------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 88         |\n","|    iterations         | 3800       |\n","|    time_elapsed       | 214        |\n","|    total_timesteps    | 19000      |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | 0.0718     |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3799       |\n","|    policy_loss        | 197        |\n","|    reward             | -2.0338488 |\n","|    std                | 1.01       |\n","|    value_loss         | 24.6       |\n","--------------------------------------\n","--------------------------------------\n","| time/                 |            |\n","|    fps                | 89         |\n","|    iterations         | 3900       |\n","|    time_elapsed       | 218        |\n","|    total_timesteps    | 19500      |\n","| train/                |            |\n","|    entropy_loss       | -41.3      |\n","|    explained_variance | 0          |\n","|    learning_rate      | 0.0007     |\n","|    n_updates          | 3899       |\n","|    policy_loss        | 168        |\n","|    reward             | -3.0972729 |\n","|    std                | 1.01       |\n","|    value_loss         | 20.8       |\n","--------------------------------------\n","Episode: 9\n","-------------------------------------\n","| time/                 |           |\n","|    fps                | 88        |\n","|    iterations         | 4000      |\n","|    time_elapsed       | 225       |\n","|    total_timesteps    | 20000     |\n","| train/                |           |\n","|    entropy_loss       | -41.3     |\n","|    explained_variance | 0         |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 3999      |\n","|    policy_loss        | -93.1     |\n","|    reward             | 1.9644926 |\n","|    std                | 1.01      |\n","|    value_loss         | 5.09      |\n","-------------------------------------\n","======a2c Validation from:  2023-10-05 to  2024-01-05\n","Episode: 1\n","a2c Sharpe Ratio:  0.5245817944056743\n","======ddpg Training========\n","{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n","Using cuda device\n","Logging to tensorboard_log//ddpg/ddpg_567_1\n","Episode: 11\n","Episode: 12\n","Episode: 13\n","Episode: 14\n","-----------------------------------\n","| time/              |            |\n","|    episodes        | 4          |\n","|    fps             | 57         |\n","|    time_elapsed    | 152        |\n","|    total_timesteps | 8816       |\n","| train/             |            |\n","|    actor_loss      | -15.9      |\n","|    critic_loss     | 2.21       |\n","|    learning_rate   | 0.0005     |\n","|    n_updates       | 8715       |\n","|    reward          | 0.73073775 |\n","-----------------------------------\n","Episode: 15\n","day: 2203, episode: 15\n","begin_total_asset: 1000000.00\n","end_total_asset: 2359124.07\n","total_reward: 1359124.07\n","total_cost: 998.99\n","total_trades: 55075\n","Sharpe: 0.628\n","=================================\n","Episode: 16\n","Episode: 17\n","Episode: 18\n","-----------------------------------\n","| time/              |            |\n","|    episodes        | 8          |\n","|    fps             | 57         |\n","|    time_elapsed    | 308        |\n","|    total_timesteps | 17632      |\n","| train/             |            |\n","|    actor_loss      | -4.96      |\n","|    critic_loss     | 1.98       |\n","|    learning_rate   | 0.0005     |\n","|    n_updates       | 17531      |\n","|    reward          | 0.73073775 |\n","-----------------------------------\n","Episode: 19\n","======ddpg Validation from:  2023-10-05 to  2024-01-05\n","Episode: 1\n","ddpg Sharpe Ratio:  0.5847587824455742\n","======ppo Training========\n","{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n","Using cuda device\n","Logging to tensorboard_log//ppo/ppo_567_1\n","-----------------------------------\n","| time/              |            |\n","|    fps             | 93         |\n","|    iterations      | 1          |\n","|    time_elapsed    | 21         |\n","|    total_timesteps | 2048       |\n","| train/             |            |\n","|    reward          | -0.6257245 |\n","-----------------------------------\n","Episode: 21\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 91          |\n","|    iterations           | 2           |\n","|    time_elapsed         | 44          |\n","|    total_timesteps      | 4096        |\n","| train/                  |             |\n","|    approx_kl            | 0.016446438 |\n","|    clip_fraction        | 0.212       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.2       |\n","|    explained_variance   | -0.00809    |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 4.29        |\n","|    n_updates            | 10          |\n","|    policy_gradient_loss | -0.0329     |\n","|    reward               | -1.0172664  |\n","|    std                  | 1           |\n","|    value_loss           | 18.6        |\n","-----------------------------------------\n","Episode: 22\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 93          |\n","|    iterations           | 3           |\n","|    time_elapsed         | 65          |\n","|    total_timesteps      | 6144        |\n","| train/                  |             |\n","|    approx_kl            | 0.015588555 |\n","|    clip_fraction        | 0.179       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.3       |\n","|    explained_variance   | -0.00508    |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 9.25        |\n","|    n_updates            | 20          |\n","|    policy_gradient_loss | -0.0309     |\n","|    reward               | 1.8190585   |\n","|    std                  | 1           |\n","|    value_loss           | 16.6        |\n","-----------------------------------------\n","Episode: 23\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 92          |\n","|    iterations           | 4           |\n","|    time_elapsed         | 88          |\n","|    total_timesteps      | 8192        |\n","| train/                  |             |\n","|    approx_kl            | 0.014422648 |\n","|    clip_fraction        | 0.204       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.3       |\n","|    explained_variance   | -0.00183    |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 6.64        |\n","|    n_updates            | 30          |\n","|    policy_gradient_loss | -0.0311     |\n","|    reward               | 0.17850801  |\n","|    std                  | 1.01        |\n","|    value_loss           | 15          |\n","-----------------------------------------\n","Episode: 24\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 93          |\n","|    iterations           | 5           |\n","|    time_elapsed         | 109         |\n","|    total_timesteps      | 10240       |\n","| train/                  |             |\n","|    approx_kl            | 0.017832845 |\n","|    clip_fraction        | 0.21        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.4       |\n","|    explained_variance   | -0.00589    |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 7.69        |\n","|    n_updates            | 40          |\n","|    policy_gradient_loss | -0.0346     |\n","|    reward               | 0.60852164  |\n","|    std                  | 1.01        |\n","|    value_loss           | 18          |\n","-----------------------------------------\n","Episode: 25\n","day: 2203, episode: 25\n","begin_total_asset: 1000000.00\n","end_total_asset: 2015914.79\n","total_reward: 1015914.79\n","total_cost: 362170.36\n","total_trades: 58354\n","Sharpe: 0.530\n","=================================\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 91          |\n","|    iterations           | 6           |\n","|    time_elapsed         | 133         |\n","|    total_timesteps      | 12288       |\n","| train/                  |             |\n","|    approx_kl            | 0.017793402 |\n","|    clip_fraction        | 0.208       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.4       |\n","|    explained_variance   | 0.00899     |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 6.07        |\n","|    n_updates            | 50          |\n","|    policy_gradient_loss | -0.0302     |\n","|    reward               | 1.5528761   |\n","|    std                  | 1.01        |\n","|    value_loss           | 18.5        |\n","-----------------------------------------\n","Episode: 26\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 92         |\n","|    iterations           | 7          |\n","|    time_elapsed         | 154        |\n","|    total_timesteps      | 14336      |\n","| train/                  |            |\n","|    approx_kl            | 0.01598082 |\n","|    clip_fraction        | 0.184      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -41.5      |\n","|    explained_variance   | 0.0659     |\n","|    learning_rate        | 0.00025    |\n","|    loss                 | 5.76       |\n","|    n_updates            | 60         |\n","|    policy_gradient_loss | -0.0302    |\n","|    reward               | 1.3788816  |\n","|    std                  | 1.01       |\n","|    value_loss           | 13.7       |\n","----------------------------------------\n","Episode: 27\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 92          |\n","|    iterations           | 8           |\n","|    time_elapsed         | 176         |\n","|    total_timesteps      | 16384       |\n","| train/                  |             |\n","|    approx_kl            | 0.019193169 |\n","|    clip_fraction        | 0.187       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.5       |\n","|    explained_variance   | 0.0676      |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 8.47        |\n","|    n_updates            | 70          |\n","|    policy_gradient_loss | -0.0243     |\n","|    reward               | -1.7561651  |\n","|    std                  | 1.01        |\n","|    value_loss           | 20          |\n","-----------------------------------------\n","Episode: 28\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 92          |\n","|    iterations           | 9           |\n","|    time_elapsed         | 198         |\n","|    total_timesteps      | 18432       |\n","| train/                  |             |\n","|    approx_kl            | 0.020578008 |\n","|    clip_fraction        | 0.213       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -41.6       |\n","|    explained_variance   | -0.00785    |\n","|    learning_rate        | 0.00025     |\n","|    loss                 | 5.66        |\n","|    n_updates            | 80          |\n","|    policy_gradient_loss | -0.0219     |\n","|    reward               | 0.8778263   |\n","|    std                  | 1.02        |\n","|    value_loss           | 18.9        |\n","-----------------------------------------\n","Episode: 29\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 92           |\n","|    iterations           | 10           |\n","|    time_elapsed         | 220          |\n","|    total_timesteps      | 20480        |\n","| train/                  |              |\n","|    approx_kl            | 0.019597003  |\n","|    clip_fraction        | 0.226        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -41.6        |\n","|    explained_variance   | 0.0121       |\n","|    learning_rate        | 0.00025      |\n","|    loss                 | 4.41         |\n","|    n_updates            | 90           |\n","|    policy_gradient_loss | -0.0254      |\n","|    reward               | -0.004502793 |\n","|    std                  | 1.02         |\n","|    value_loss           | 14.3         |\n","------------------------------------------\n","======ppo Validation from:  2023-10-05 to  2024-01-05\n","Episode: 1\n","ppo Sharpe Ratio:  0.5242179546011245\n","======Best Model Retraining from:  2015-01-01 to  2024-01-05\n","======Trading from:  2024-01-05 to  2024-04-08\n","Used Model:  <stable_baselines3.ddpg.ddpg.DDPG object at 0x7a99a773b3d0>\n","Episode: 1\n","Ensemble Strategy took:  101.92540356318156  minutes\n"]}],"source":["df_summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n","                                                 PPO_model_kwargs,\n","                                                 DDPG_model_kwargs,\n","                                                 timesteps_dict) if if_using_ensemble else None"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1719031715621,"user":{"displayName":"Fai Gar","userId":"14965246322519692067"},"user_tz":-420},"id":"7ub0E28nyiLB","outputId":"b8388077-ec11-4ec7-8260-bb09f4023b68"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"df_summary\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Iter\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 126,\n        \"max\": 567,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          189,\n          441,\n          126\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Val Start\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"2022-04-04\",\n          \"2023-04-05\",\n          \"2022-01-03\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Val End\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"2022-07-06\",\n          \"2023-07-07\",\n          \"2022-04-04\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Model Used\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"A2C\",\n          \"DDPG\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"A2C Sharpe\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": -0.34852958410513796,\n        \"max\": 0.5245817944056743,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          -0.13300719086992657,\n          -0.06649924689916284\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PPO Sharpe\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": -0.32861983431148556,\n        \"max\": 0.5242179546011245,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          -0.2267923621418156,\n          -0.04620413403541566\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DDPG Sharpe\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": -0.2264847842521231,\n        \"max\": 0.5847587824455742,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          -0.2264847842521231,\n          0.02913844642150749\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"df_summary"},"text/html":["\n","  <div id=\"df-7ff6d1de-e7eb-4846-953e-840ad4303215\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Iter</th>\n","      <th>Val Start</th>\n","      <th>Val End</th>\n","      <th>Model Used</th>\n","      <th>A2C Sharpe</th>\n","      <th>PPO Sharpe</th>\n","      <th>DDPG Sharpe</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>126</td>\n","      <td>2022-01-03</td>\n","      <td>2022-04-04</td>\n","      <td>DDPG</td>\n","      <td>-0.141601</td>\n","      <td>-0.193884</td>\n","      <td>-0.019706</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>189</td>\n","      <td>2022-04-04</td>\n","      <td>2022-07-06</td>\n","      <td>A2C</td>\n","      <td>-0.133007</td>\n","      <td>-0.226792</td>\n","      <td>-0.226485</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>252</td>\n","      <td>2022-07-06</td>\n","      <td>2022-10-04</td>\n","      <td>DDPG</td>\n","      <td>-0.261174</td>\n","      <td>-0.32862</td>\n","      <td>-0.022517</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>315</td>\n","      <td>2022-10-04</td>\n","      <td>2023-01-04</td>\n","      <td>A2C</td>\n","      <td>0.299137</td>\n","      <td>0.255318</td>\n","      <td>0.225312</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>378</td>\n","      <td>2023-01-04</td>\n","      <td>2023-04-05</td>\n","      <td>DDPG</td>\n","      <td>0.034476</td>\n","      <td>-0.117529</td>\n","      <td>0.041095</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>441</td>\n","      <td>2023-04-05</td>\n","      <td>2023-07-07</td>\n","      <td>DDPG</td>\n","      <td>-0.066499</td>\n","      <td>-0.046204</td>\n","      <td>0.029138</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>504</td>\n","      <td>2023-07-07</td>\n","      <td>2023-10-05</td>\n","      <td>DDPG</td>\n","      <td>-0.34853</td>\n","      <td>-0.230318</td>\n","      <td>0.013958</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>567</td>\n","      <td>2023-10-05</td>\n","      <td>2024-01-05</td>\n","      <td>DDPG</td>\n","      <td>0.524582</td>\n","      <td>0.524218</td>\n","      <td>0.584759</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ff6d1de-e7eb-4846-953e-840ad4303215')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-7ff6d1de-e7eb-4846-953e-840ad4303215 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-7ff6d1de-e7eb-4846-953e-840ad4303215');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-53bfd861-f933-4e25-99c8-86d94e3c8f06\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-53bfd861-f933-4e25-99c8-86d94e3c8f06')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-53bfd861-f933-4e25-99c8-86d94e3c8f06 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_05e8645f-28f3-46f2-a499-44df16054ff7\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_summary')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_05e8645f-28f3-46f2-a499-44df16054ff7 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df_summary');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["  Iter   Val Start     Val End Model Used A2C Sharpe PPO Sharpe DDPG Sharpe\n","0  126  2022-01-03  2022-04-04       DDPG  -0.141601  -0.193884   -0.019706\n","1  189  2022-04-04  2022-07-06        A2C  -0.133007  -0.226792   -0.226485\n","2  252  2022-07-06  2022-10-04       DDPG  -0.261174   -0.32862   -0.022517\n","3  315  2022-10-04  2023-01-04        A2C   0.299137   0.255318    0.225312\n","4  378  2023-01-04  2023-04-05       DDPG   0.034476  -0.117529    0.041095\n","5  441  2023-04-05  2023-07-07       DDPG  -0.066499  -0.046204    0.029138\n","6  504  2023-07-07  2023-10-05       DDPG   -0.34853  -0.230318    0.013958\n","7  567  2023-10-05  2024-01-05       DDPG   0.524582   0.524218    0.584759"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["df_summary if if_using_ensemble else None"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
